{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Upload AirNow data\n",
    "------------------\n",
    "\n",
    "Processes the Airnow hourly data files located in `./AirNow` and uploads to ESDR.\n",
    "\n",
    "Reports to stat.createlab.org as `Airnow Hourly Data - Upload`\n",
    "\n",
    "[Format documentation](http://www.airnowapi.org/docs/HourlyDataFactSheet.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import json, os, dateutil, re, requests, subprocess, datetime, glob, stat, codecs, sys\n",
    "\n",
    "from dateutil import rrule, tz, parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Boilerplate to load utils.ipynb\n",
    "# See https://github.com/CMU-CREATE-Lab/python-utils/blob/master/utils.ipynb\n",
    "\n",
    "\n",
    "def exec_ipynb(filename_or_url):\n",
    "    nb = (requests.get(filename_or_url).json() if re.match(r'https?:', filename_or_url) else json.load(open(filename_or_url)))\n",
    "    if(nb['nbformat'] >= 4):\n",
    "        src = [''.join(cell['source']) for cell in nb['cells'] if cell['cell_type'] == 'code']\n",
    "    else:\n",
    "        src = [''.join(cell['input']) for cell in nb['worksheets'][0]['cells'] if cell['cell_type'] == 'code']\n",
    "\n",
    "    tmpname = '/tmp/%s-%s-%d.py' % (os.path.basename(filename_or_url),\n",
    "                                    datetime.datetime.now().strftime('%Y%m%d%H%M%S%f'),\n",
    "                                    os.getpid())\n",
    "    src = '\\n\\n\\n'.join(src)\n",
    "    open(tmpname, 'w').write(src)\n",
    "    code = compile(src, tmpname, 'exec')\n",
    "    exec(code, globals())\n",
    "\n",
    "\n",
    "exec_ipynb('./python-utils/utils.ipynb')\n",
    "exec_ipynb('./python-utils/esdr-library.ipynb')\n",
    "exec_ipynb('./airnow-common.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "STAT_SERVICE_NAME = 'Airnow Hourly Data - Upload'\n",
    "STAT_HOSTNAME = 'hal21'\n",
    "STAT_SHORTNAME = 'airnow-hourly-data-upload'\n",
    "\n",
    "UPTIME_VALID_TIME_PERIOD_SECS = 60 * 60 * 2 # two hours\n",
    "\n",
    "ESDR_MONITORING_SITE_LOCATION_DEVICES_AND_FEEDS_JSON_FILENAME = 'esdr_monitoring_site_location_devices_and_feeds.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "Stat.set_service(STAT_SERVICE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Accumulate data from multiple files\n",
    "# Assumes accumulation in time order\n",
    "accumulated = {}\n",
    "accumulated_files = {}\n",
    "\n",
    "def clear_accumulated():\n",
    "    global accumulated, accumulated_files\n",
    "    accumulated = {}\n",
    "    accumulated_files = {}\n",
    "\n",
    "def accumulate_airnow_file(src):\n",
    "    print('Accumulating airnow file %s' % src)\n",
    "    src_epoch_timestamp = os.path.getmtime(src)\n",
    "    dt = datetime.datetime.strptime(os.path.basename(src), '%Y%m%d%H.dat')\n",
    "    # Offset epoch_time by 1800 seconds to be in middle of hour-long sample\n",
    "    epoch_time = (dt - datetime.datetime(1970, 1, 1)).total_seconds() + 1800\n",
    "\n",
    "    nsamples = 0\n",
    "\n",
    "    with open(src, 'r', encoding='cp437') as airnow:\n",
    "        lineno = 0\n",
    "        error_count = 0\n",
    "        for record in airnow:\n",
    "            lineno += 1\n",
    "            try:\n",
    "                (_, _, id, _, _, type, units, value, _) = record.split('|')\n",
    "            except:\n",
    "                sys.stderr.write('Problem parsing %s line %d, skipping\\n' % (src, lineno))\n",
    "                sys.stderr.write('Line \"%s\"\\n' % record)\n",
    "                error_count += 1\n",
    "            type = re.sub(r'\\W', '_', type) # Replace non-word chars with _;  e.g. PM2.5 becomes PM2_5\n",
    "\n",
    "            if not id in accumulated:\n",
    "                accumulated[id] = {}\n",
    "\n",
    "            if not type in accumulated[id]:\n",
    "                accumulated[id][type] = []\n",
    "\n",
    "            accumulated[id][type].append([epoch_time, float(value)])\n",
    "            nsamples += 1\n",
    "        if error_count > 5:\n",
    "            raise Exception('Too many parse errors (%d) reading %s, aborting' % (error_count, src))\n",
    "\n",
    "    if error_count > 0:\n",
    "        Stat.warning('Read %d records from %s (%d error(s))' % (nsamples, src, error_count), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "    else:\n",
    "        Stat.debug('Read %d records from %s (%d error(s))' % (nsamples, src, error_count), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "    accumulated_files[src] = src_epoch_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "sites_cached = None\n",
    "\n",
    "def refresh_site_info_cache():\n",
    "    global sites_cached\n",
    "    with open(AirnowCommon.DATA_DIRECTORY + '/monitoring_site_locations.json', 'r') as f:\n",
    "        sites_cached = json.load(f)\n",
    "\n",
    "def get_site_info(site_id):\n",
    "    global sites_cached\n",
    "    if not sites_cached:\n",
    "        refresh_site_info_cache()\n",
    "\n",
    "    try:\n",
    "        return sites_cached['sites'][site_id]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# print(json.dumps(get_site_info('420030008'), sort_keys=True, indent=3))  # Lawrenceville aka \"BAPC 301 39TH STREET BLDG #7 AirNow\"\n",
    "# print(json.dumps(get_site_info('000050121'), sort_keys=True, indent=3))  # Meteorological Service of Canada\"\n",
    "# print(json.dumps(get_site_info('044201010'), sort_keys=True, indent=3))  # null\n",
    "# print(json.dumps(get_site_info('060870007'), sort_keys=True, indent=3))  # Santa Cruz AMS\n",
    "# print(json.dumps(get_site_info('033211050'), sort_keys=True, indent=3))  # null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "esdr = None\n",
    "airnow_product = None\n",
    "esdr_monitoring_site_devices = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def get_airnow_product():\n",
    "    global esdr, airnow_product\n",
    "    if not esdr:\n",
    "        esdr = Esdr('esdr-auth-airnow-uploader.json', user_agent='esdr-library.py['+STAT_SERVICE_NAME+']')\n",
    "    if not airnow_product:\n",
    "        # esdr.create_product('AirNow', 'AirNow', 'EPA and Sonoma Tech', 'Real-time feeds from EPA/STI AirNow')\n",
    "        airnow_product = esdr.get_product_by_name('AirNow')\n",
    "    return airnow_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def refresh_esdr_monitoring_site_device_cache():\n",
    "    global esdr_monitoring_site_devices\n",
    "    with open(AirnowCommon.DATA_DIRECTORY + '/' + ESDR_MONITORING_SITE_LOCATION_DEVICES_AND_FEEDS_JSON_FILENAME, 'r') as f:\n",
    "        esdr_monitoring_site_devices = json.load(f)\n",
    "\n",
    "def get_esdr_monitoring_site_device(serialNumber):\n",
    "    global airnow_product, esdr_monitoring_site_devices\n",
    "    if not airnow_product:\n",
    "        airnow_product = get_airnow_product()\n",
    "    if not esdr_monitoring_site_devices:\n",
    "        refresh_esdr_monitoring_site_device_cache()\n",
    "\n",
    "    if serialNumber in esdr_monitoring_site_devices:\n",
    "        # get a copy of the device\n",
    "        device = esdr_monitoring_site_devices[serialNumber].copy()\n",
    "\n",
    "        # add the serial number and product id\n",
    "        device['serialNumber'] = serialNumber\n",
    "        device['productId'] = airnow_product['id']\n",
    "        return device\n",
    "\n",
    "    return None\n",
    "\n",
    "def get_esdr_monitoring_site_feed(device, lat, lng):\n",
    "    if device and lat and lng:\n",
    "        if 'feeds' in device:\n",
    "            for feed in device['feeds']:\n",
    "                if float(lat) == feed['lat'] and float(lng) == feed['lng']:\n",
    "                    return feed\n",
    "    return None\n",
    "\n",
    "# print(get_esdr_monitoring_site_device('no such site'))  # None\n",
    "# print(get_esdr_monitoring_site_device('033211050'))     # None\n",
    "# print(get_esdr_monitoring_site_device('010972005'))     # {'feeds': [{'id': 2264, 'lat': 30.4744, 'lng': -88.1411}], 'id': 2264, 'name': 'BAYROAD', 'serialNumber': '010972005', 'productId': 11}\n",
    "# print(get_esdr_monitoring_site_device('060870007'))     # {'feeds': [{'id': 34142, 'lat': 36.98332, 'lng': -121.98822}, {'id': 2511, 'lat': 36.985802, 'lng': -121.993103}], 'id': 2511, 'name': 'Santa Cruz AMS', 'serialNumber': '060870007', 'productId': 11}\n",
    "# print(get_esdr_monitoring_site_feed(get_esdr_monitoring_site_device('060870007'),36.98332,-121.98822))        # {'id': 34142, 'lat': 36.98332, 'lng': -121.98822}\n",
    "# print(get_esdr_monitoring_site_feed(get_esdr_monitoring_site_device('060870007'),36.985802,-121.993103))      # {'id': 2511, 'lat': 36.985802, 'lng': -121.993103}\n",
    "# print(get_esdr_monitoring_site_feed(get_esdr_monitoring_site_device('060870007'),'36.98332','-121.98822'))    # {'id': 34142, 'lat': 36.98332, 'lng': -121.98822}\n",
    "# print(get_esdr_monitoring_site_feed(get_esdr_monitoring_site_device('060870007'),'36.985802','-121.993103'))  # {'id': 2511, 'lat': 36.985802, 'lng': -121.993103}\n",
    "# print(get_esdr_monitoring_site_feed(get_esdr_monitoring_site_device('060870007'),'36.9833','-121.9882'))      # None\n",
    "# print(get_esdr_monitoring_site_feed(get_esdr_monitoring_site_device('060870007'),'36.9858','-121.9931'))      # None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def upload_site(site_id):\n",
    "    global esdr, airnow_product\n",
    "    if not esdr:\n",
    "        esdr = Esdr('esdr-auth-airnow-uploader.json', user_agent='esdr-library.py['+STAT_SERVICE_NAME+']')\n",
    "    if not airnow_product:\n",
    "        airnow_product = get_airnow_product()\n",
    "\n",
    "    # try to get the device from the cache\n",
    "    device = get_esdr_monitoring_site_device(site_id)\n",
    "\n",
    "    site_info = get_site_info(site_id)\n",
    "\n",
    "    if not device:\n",
    "        if not site_info:\n",
    "            Stat.warning('Cannot create device for site %s because no information can be found for it.  Skipping.' % (site_id), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "            return\n",
    "        Stat.info('Failed to find cached device for site %s. Will get from ESDR, creating if necessary.' % (site_id), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "        device = esdr.get_or_create_device(airnow_product, serial_number=site_id, name=site_info['site name'])\n",
    "\n",
    "    if device:\n",
    "        # Find the feed, matching on lat/lon for the case where the site has moved.  We start by\n",
    "        # checking the cache, which has all known feeds for the devices, and the feeds are sorted\n",
    "        # in reverse chronological order, so we should typically (always?) find a hit on the first\n",
    "        # try.  If we don't find the feed in the cache, then fallback to esdr.get_feed(), which\n",
    "        # also will match by lat/lon if there are multiple feeds for the device\n",
    "        lat = float(site_info['latitude']) if site_info else None\n",
    "        lon = float(site_info['longitude']) if site_info else None\n",
    "        feed = get_esdr_monitoring_site_feed(device, lat, lon)\n",
    "\n",
    "        # load from ESDR if we couldn't find it in the cache\n",
    "        if not feed:\n",
    "            Stat.info(\"Failed to find cached feed for device %d with (lat,lng) of (%s, %s). Will try to fetch from ESDR.\" % (device['id'], lat, lon), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "            feed = esdr.get_feed(device, lat=lat, lon=lon)\n",
    "\n",
    "        # if we couldn't load from ESDR, then create it\n",
    "        if not feed:\n",
    "            if not site_info:\n",
    "                Stat.warning('Cannot create feed for site %s because no information can be found for it.  Skipping.' % (site_id), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "                return\n",
    "            Stat.info(\"Failed to load feed for device %d with (lat,lng) of (%s, %s) from ESDR. Will try to create in ESDR.\" % (device['id'], lat, lon), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "            feed = esdr.get_or_create_feed(device, lat=lat, lon=lon)\n",
    "\n",
    "        if site_id in accumulated:\n",
    "            channels = accumulated[site_id]\n",
    "            channel_to_num_samples_uploaded = {}\n",
    "\n",
    "            for channel in channels:\n",
    "                channel_to_num_samples_uploaded[channel] = 0\n",
    "                try:\n",
    "                    esdr.upload(feed, {\n",
    "                        'channel_names': [channel],\n",
    "                        'data': channels[channel]\n",
    "                    })\n",
    "                    channel_to_num_samples_uploaded[channel] = len(channels[channel])\n",
    "                    print('%s/%s, %s: Uploaded %d samples to feed ID %d.' % (site_id, device['name'], channel, len(channels[channel]), feed['id']))\n",
    "                    #Stat.info('%s/%s, %s: Uploaded %d samples.' % (site_id, device['name'], channel, len(channels[channel])), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "                except requests.HTTPError as e:\n",
    "                    Stat.warning('%s/%s, %s: Failed to upload %d samples (HTTP %d).' %\n",
    "                                 (site_id, device['name'], channel, len(channels[channel]), e.response.status_code), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "                except:\n",
    "                    Stat.warning('%s/%s, %s: Failed to upload %d samples.' %\n",
    "                                 (site_id, device['name'], channel, len(channels[channel])), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "            # build per-channel upload stats\n",
    "            samples_uploaded_per_channel = []\n",
    "            for item in channel_to_num_samples_uploaded.items():\n",
    "                samples_uploaded_per_channel.append(':'.join(map(str,item)))\n",
    "\n",
    "            per_channel_stats = ', '.join(samples_uploaded_per_channel)\n",
    "\n",
    "            Stat.info('%s/%s: Uploaded %d channels to feed ID %d (%s)' % (site_id, device['name'], len(channels), feed['id'], per_channel_stats), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "        else:\n",
    "            Stat.warning('%s/%s: No accumulated data found. Skipping.' % (site_id, device['name']), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "    else:\n",
    "        Stat.warning('Failed to find or create device for site %s.  Skipping.' % (site_id), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "        return\n",
    "\n",
    "#upload_site('000010401')\n",
    "#upload_site('000051501')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "def upload_check_path(src):\n",
    "    return 'upload-airnow-to-esdr/uploaded-' + os.path.basename(src)\n",
    "\n",
    "def upload_accumulated():\n",
    "\n",
    "    refresh_site_info_cache()\n",
    "\n",
    "    i = 0\n",
    "    for site_id in sorted(accumulated.keys()):\n",
    "        print('Uploading site %d' % i)\n",
    "        i += 1\n",
    "        upload_site(site_id)\n",
    "    for src in sorted(accumulated_files):\n",
    "        check_path = upload_check_path(src)\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(check_path))\n",
    "        except:\n",
    "            pass\n",
    "        open(check_path + '.tmp', 'w').close()\n",
    "        src_epoch_time = accumulated_files[src]\n",
    "        os.utime(check_path + '.tmp', (src_epoch_time, src_epoch_time))\n",
    "        os.rename(check_path + '.tmp', check_path)\n",
    "        Stat.debug('Uploaded %s to ESDR' % (src), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "    clear_accumulated()\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "def process_all():\n",
    "    Stat.info('Uploading hourly Airnow data to ESDR...', host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "    before = time.time()\n",
    "    clear_accumulated()\n",
    "    for src in sorted(glob.glob('AirNow/[0-9]*.dat')):\n",
    "        if len(accumulated_files) == 1000:\n",
    "            upload_accumulated()\n",
    "        try:\n",
    "            if os.path.getmtime(src) == os.path.getmtime(upload_check_path(src)):\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        accumulate_airnow_file(src)\n",
    "    nsites = upload_accumulated()\n",
    "    after = time.time()\n",
    "    Stat.up('Done uploading %d sites to ESDR' % nsites, details='Took %.1f minutes' % ((after - before) / 60), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME, valid_for_secs=UPTIME_VALID_TIME_PERIOD_SECS)\n",
    "\n",
    "def process_all_forever():\n",
    "    while True:\n",
    "        process_all()\n",
    "        sleep_until_next_period(1 * 60)  # start up again within 1 minute\n",
    "\n",
    "process_all_forever()\n",
    "#process_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}