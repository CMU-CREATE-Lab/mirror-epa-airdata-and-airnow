{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import AirNow data\n",
    "------------------\n",
    "\n",
    "[Format documentation](http://www.airnowapi.org/docs/HourlyDataFactSheet.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs, datetime, glob, os, re\n",
    "\n",
    "def exec_ipynb(filename_or_url):\n",
    "    nb = (urllib2.urlopen(filename_or_url) if re.match(r'https?:', filename_or_url) else open(filename_or_url)).read()\n",
    "    jsonNb = json.loads(nb)\n",
    "    #check for the modified formatting of Jupyter Notebook v4\n",
    "    if(jsonNb['nbformat'] == 4):\n",
    "        exec '\\n'.join([''.join(cell['source']) for cell in jsonNb['cells'] if cell['cell_type'] == 'code']) in globals()\n",
    "    else:\n",
    "        exec '\\n'.join([''.join(cell['input']) for cell in jsonNb['worksheets'][0]['cells'] if cell['cell_type'] == 'code']) in globals()\n",
    "\n",
    "exec_ipynb('python-utils/esdr-library.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Accumulate data from multiple files\n",
    "# Assumes accumulation in time order\n",
    "accumulated = {}\n",
    "accumulated_files = {}\n",
    "\n",
    "def clear_accumulated():\n",
    "    global accumulated, accumulated_files\n",
    "    accumulated = {}\n",
    "    accumulated_files = {}\n",
    "\n",
    "def accumulate_airnow_file(src):\n",
    "    print 'Accumulating airnow file %s' % src\n",
    "    src_epoch_timestamp = os.path.getmtime(src)\n",
    "    dt = datetime.datetime.strptime(os.path.basename(src), '%Y%m%d%H.dat')\n",
    "    # Offset epoch_time by 1800 seconds to be in middle of hour-long sample\n",
    "    epoch_time = (dt - datetime.datetime(1970, 1, 1)).total_seconds() + 1800\n",
    "    \n",
    "    nsamples = 0\n",
    "\n",
    "    with open(src, 'r') as airnow:\n",
    "        for record in airnow:\n",
    "            (_, _, id, _, _, type, units, value, _) = record.split('|')\n",
    "            type = re.sub(r'\\W', '_', type) # Replace non-word chars with _;  e.g. PM2.5 becomes PM2_5\n",
    "            \n",
    "            if not id in accumulated:\n",
    "                accumulated[id] = {}\n",
    "            \n",
    "            if not type in accumulated[id]:\n",
    "                accumulated[id][type] = []\n",
    "                \n",
    "            accumulated[id][type].append([epoch_time, float(value)])\n",
    "            nsamples += 1\n",
    "    \n",
    "    print 'Read %d samples from %s' % (nsamples, src)\n",
    "    accumulated_files[src] = src_epoch_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Documentation for monitoring_site_locations.dat\n",
    "# http://www.airnowapi.org/docs/MonitoringSiteFactSheet.pdf\n",
    "\n",
    "def read_sites():\n",
    "    field_names = ('AQSID|parameter name|site code|site name|status|' +\n",
    "                   'agency id|agency name|EPA region|latitude|longitude|' +\n",
    "                   'elevation|GMT offset|country code|CMSA code|CMSA name|' + \n",
    "                   'MSA code|MSA name|state code|state name|county code|' +\n",
    "                   'county name|city code|city name').split('|')\n",
    "\n",
    "    sites = {}\n",
    "\n",
    "    # monitoring_site_locations.dat has non-ASCII characters, in the archaic Original IBM PC 8-bit charset\n",
    "    # known today as Code page 437.  Translate to unicode during read\n",
    "    source = 'AirNow/monitoring_site_locations.dat'\n",
    "    data = codecs.open(source, 'r', 'cp437').read()\n",
    "    # Test: 000050121 is PARC OCÃ‰ANIE\n",
    "    \n",
    "    for line in data.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        fields = line.strip().split('|')\n",
    "        if len(field_names) != len(fields):\n",
    "            raise Exception('There are %d field names but %d fields' % (len(field_names), len(fields)))\n",
    "        channel_info = dict(zip(field_names, fields))\n",
    "        aqsid = channel_info['AQSID']\n",
    "        if not aqsid in sites:\n",
    "            sites[aqsid] = {}\n",
    "        parameter = channel_info['parameter name']\n",
    "        if parameter in sites[aqsid]:\n",
    "            raise Exception('Duplicate in monitoring_site_locations: %s:%s' % (aqsid, parameter))\n",
    "        sites[aqsid][parameter] = channel_info\n",
    "    \n",
    "    print 'Read %d sites from %s' % (len(sites), source)\n",
    "    return sites\n",
    "\n",
    "sites_cached = None\n",
    "\n",
    "def get_site_channel_info(site_id):\n",
    "    global sites_cached\n",
    "    if not sites_cached:\n",
    "        sites_cached = read_sites()\n",
    "    try:\n",
    "        return sites_cached[site_id]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Use info from first channel for overall site\n",
    "def get_site_info(site_id):\n",
    "    channel_info = get_site_channel_info(site_id)\n",
    "    if channel_info == None:\n",
    "        return None\n",
    "    else:\n",
    "        return channel_info[sorted(channel_info.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "esdr = None\n",
    "airnow_product = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def upload_site(site_id):\n",
    "    global esdr, airnow_product\n",
    "    if not airnow_product:\n",
    "        esdr = Esdr('esdr-auth-airnow-uploader.json')\n",
    "        # esdr.create_product('AirNow', 'AirNow', 'EPA and Sonoma Tech', 'Real-time feeds from EPA/STI AirNow')\n",
    "        airnow_product = esdr.get_product_by_name('AirNow')\n",
    "\n",
    "    device = esdr.get_device_by_serial_number(airnow_product, site_id)\n",
    "\n",
    "    if not device:\n",
    "        site_info = get_site_info(site_id)\n",
    "        if not site_info:\n",
    "            print 'NOTIFY(WARNING): Cannot create site %s because no information can be found for it.  Skipping.' % site_id\n",
    "            return\n",
    "        device = esdr.get_or_create_device(airnow_product, serial_number=site_id, name=site_info['site name'])\n",
    "\n",
    "    feed = esdr.get_feed(device)\n",
    "    \n",
    "    if not feed:\n",
    "        site_info = get_site_info(site_id)\n",
    "        feed = esdr.get_or_create_feed(device, lat=float(site_info['latitude']), lon=float(site_info['longitude']))\n",
    "    \n",
    "    channels = accumulated[site_id]\n",
    "\n",
    "    for channel in channels:\n",
    "        esdr.upload(feed, {\n",
    "            'channel_names': [channel],\n",
    "            'data': channels[channel]\n",
    "        })\n",
    "        print '%s/%s, %s: Uploaded %d samples.' % (site_id, device['name'], channel, len(channels[channel]))\n",
    "\n",
    "#upload_site('000010401')\n",
    "#upload_site('000051501')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def upload_check_path(src):\n",
    "    return 'upload-airnow-to-esdr/uploaded-' + os.path.basename(src)\n",
    "\n",
    "def upload_accumulated():\n",
    "    for site_id in sorted(accumulated.keys()):\n",
    "        upload_site(site_id)\n",
    "    for src in sorted(accumulated_files):\n",
    "        check_path = upload_check_path(src)\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(check_path))\n",
    "        except:\n",
    "            pass\n",
    "        open(check_path + '.tmp', 'w').close()\n",
    "        src_epoch_time = accumulated_files[src]\n",
    "        os.utime(check_path + '.tmp', (src_epoch_time, src_epoch_time))\n",
    "        os.rename(check_path + '.tmp', check_path)\n",
    "        print 'STATUS(SUCCESS): uploaded %s to ESDR' % src\n",
    "    clear_accumulated()\n",
    "\n",
    "def process_all():\n",
    "    clear_accumulated()\n",
    "    for src in sorted(glob.glob('AirNow/[0-9]*.dat')):\n",
    "        if len(accumulated_files) == 1000:\n",
    "            upload_accumulated()\n",
    "        try:\n",
    "            if os.path.getmtime(src) == os.path.getmtime(upload_check_path(src)):\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        accumulate_airnow_file(src)\n",
    "    upload_accumulated()\n",
    "        \n",
    "#for site in sorted(accumulated.keys())[0:10]:\n",
    "#    print site\n",
    "#    print len(accumulated[site])\n",
    "\n",
    "#site_id = '000051501'\n",
    "#channels = accumulated[site_id]\n",
    "#channels  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "process_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
