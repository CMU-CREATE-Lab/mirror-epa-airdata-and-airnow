{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Mirror Reporting Area Locations\n",
    "\n",
    "Mirrors two Airnow reporting area data files, converts each to JSON, and then also merges the two into `reporting_areas.json`. Details:\n",
    "\n",
    "* The Airnow [Site_To_ReportingArea.csv](https://files.airnowtech.org/airnow/today/Site_To_ReportingArea.csv) is converted into `reporting_areas_to_sites.json`.\n",
    "* The [reporting area locations .dat file](https://files.airnowtech.org/airnow/today/reporting_area_locations_V2.dat) is converted into `reporting_area_locations.json`.\n",
    "\n",
    "We merge the files using the reporting area ID (e.g. `wa019`, `ak010`, etc). When I first started mirroring, I found a few errors such as multiple reporting areas being assigned to the same ID (see the git history for this file).  I contacted Airnow, and they assured me reporting area IDs should be unique and fixed the problems.\n",
    "\n",
    "The resulting merged JSON file, `reporting_areas.json`, is a dictionary mapping reporting area ID (e.g. `wa019`) to data about that reporting area, including a collection of monitoring site IDs associated with that reporting area.  Note that a monitoring site may be associated with more than one reporting area, and that a reporting area may actually have zero associated monitoring sites.\n",
    "\n",
    "Airnow told me that `Site_To_ReportingArea.csv` gets updated twice daily. But, from what I can tell, the `reporting_area_locations_V2.dat` file gets updated twice per hour, at 25 and 55 minutes after the hour.  So this mirror runs on the hour and half hour.\n",
    "\n",
    "Data sheet is located at https://docs.airnowapi.org/docs/ReportingAreaInformationFactSheet.pdf\n",
    "\n",
    "Reports to stat.createlab.org as `Airnow Reporting Area Locations File - Mirror`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import json, os, dateutil, re, requests, subprocess, datetime, glob, stat\n",
    "import csv\n",
    "\n",
    "from dateutil import rrule, tz, parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Boilerplate to load utils.ipynb\n",
    "# See https://github.com/CMU-CREATE-Lab/python-utils/blob/master/utils.ipynb\n",
    "\n",
    "def exec_ipynb(filename_or_url):\n",
    "    nb = (requests.get(filename_or_url).json() if re.match(r'https?:', filename_or_url) else json.load(open(filename_or_url)))\n",
    "    if(nb['nbformat'] >= 4):\n",
    "        src = [''.join(cell['source']) for cell in nb['cells'] if cell['cell_type'] == 'code']\n",
    "    else:\n",
    "        src = [''.join(cell['input']) for cell in nb['worksheets'][0]['cells'] if cell['cell_type'] == 'code']\n",
    "\n",
    "    tmpname = '/tmp/%s-%s-%d.py' % (os.path.basename(filename_or_url),\n",
    "                                    datetime.datetime.now().strftime('%Y%m%d%H%M%S%f'),\n",
    "                                    os.getpid())\n",
    "    src = '\\n\\n\\n'.join(src)\n",
    "    open(tmpname, 'w').write(src)\n",
    "    code = compile(src, tmpname, 'exec')\n",
    "    exec(code, globals())\n",
    "\n",
    "\n",
    "exec_ipynb('./python-utils/utils.ipynb')\n",
    "exec_ipynb('./airnow-common.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "MIRROR_TIME_PERIOD_SECS = 60 * 30   # every 30 minutes\n",
    "\n",
    "STAT_SERVICE_NAME = 'Airnow Reporting Area Locations File - Mirror'\n",
    "STAT_HOSTNAME = 'hal21'\n",
    "STAT_SHORTNAME = 'airnow-mirror-reporting-area-locations-file'\n",
    "\n",
    "REPORTING_AREA_LOCATIONS_DAT_FILENAME = 'reporting_area_locations_V2.dat'\n",
    "REPORTING_AREA_LOCATIONS_JSON_FILENAME = 'reporting_area_locations.json'\n",
    "\n",
    "SITE_TO_REPORTING_AREA_CSV_FILENAME = 'Site_To_ReportingArea.csv'\n",
    "REPORTING_AREAS_TO_SITES_JSON_FILENAME = 'reporting_areas_to_sites.json'\n",
    "\n",
    "REPORTING_AREAS_JSON_FILENAME = 'reporting_areas.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "Stat.set_service(STAT_SERVICE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Currently unused now that Airnow has fixed the duplications and errors surrounding reporting area IDs (see docs above, and in the git history)\n",
    "#\n",
    "# def create_reporting_area_unique_id(reporting_area_id, reporting_area_name):\n",
    "#     if reporting_area_id and reporting_area_name:\n",
    "#         stripped_id = reporting_area_id.strip()\n",
    "#         clean_name = re.sub(r'[^a-zA-Z0-9]+', '', reporting_area_name) # Strip non-alphanumeric chars\n",
    "#\n",
    "#         if len(stripped_id) > 0 and len(clean_name) > 0:\n",
    "#             return (stripped_id + '-' + clean_name).lower()\n",
    "#\n",
    "#     return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def jsonify_reporting_area_locations():\n",
    "    field_names = ('name|stateCode|countryCode|forecasts|actionDayName|lat|lng|gmtOffset|hasDST|tzLabel|dstzLabel|id|usaToday|forecastSource').split('|')\n",
    "\n",
    "    reporting_areas = {}\n",
    "\n",
    "    # The file may have non-ASCII characters, in the archaic Original IBM PC 8-bit charset\n",
    "    # known today as Code page 437.  Translate to unicode during read\n",
    "    source = AirnowCommon.DATA_DIRECTORY + '/' + REPORTING_AREA_LOCATIONS_DAT_FILENAME\n",
    "    dest = AirnowCommon.DATA_DIRECTORY + '/' + REPORTING_AREA_LOCATIONS_JSON_FILENAME\n",
    "    data =  open(source, 'r', encoding='cp437').read()\n",
    "\n",
    "    for line in data.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        fields = list(map(lambda s: s.strip(), line.split('|')))    # split on | then strip whitespace from every field\n",
    "        if len(field_names) != len(fields):\n",
    "            Stat.warning('Record has %d field names but %d fields. Skipping.' % (len(field_names), len(fields)), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "            continue\n",
    "        field_map = dict(zip(field_names, fields))\n",
    "        key = field_map['id']\n",
    "\n",
    "        if key and len(key) > 0:\n",
    "            # delete keys we don't need\n",
    "            field_map.pop('forecasts', None)\n",
    "            field_map.pop('actionDayName', None)\n",
    "            field_map.pop('usaToday', None)\n",
    "            field_map.pop('forecasts', None)\n",
    "            field_map.pop('forecastSource', None)\n",
    "\n",
    "            if field_map['hasDST'] == 'Yes':\n",
    "                field_map['hasDST'] = True\n",
    "            elif field_map['hasDST'] == 'No':\n",
    "                field_map['hasDST'] = False\n",
    "                field_map.pop('dstzLabel', None)  # no point including the daylight savings time label if they don't do DST\n",
    "\n",
    "            # convert lat/lng from string to float\n",
    "            field_map['lat'] = float(field_map['lat'])\n",
    "            field_map['lng'] = float(field_map['lng'])\n",
    "\n",
    "            # add it to the map\n",
    "            if key not in reporting_areas:\n",
    "                reporting_areas[key] = field_map\n",
    "            else:\n",
    "                Stat.warning('skipping duplicate ID [%s] for reporting area [%s, %s, %s]' % (key, field_map['name'], field_map['stateCode'], field_map['countryCode']), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "        else:\n",
    "            Stat.warning('skipping reporting area [%s, %s, %s] since it has an empty ID' % (field_map['name'], field_map['stateCode'], field_map['countryCode']), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "    Stat.debug('Read %d reporting areas from %s' % (len(reporting_areas), source), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "    # write the JSON file to disk\n",
    "    tmp = dest + '.tmp' + str(os.getpid())\n",
    "    os.makedirs(os.path.dirname(tmp), exist_ok=True)\n",
    "    with open(tmp, 'w') as json_file:\n",
    "        json.dump(reporting_areas, json_file, sort_keys=True)\n",
    "    os.rename(tmp, dest)\n",
    "\n",
    "    # make the JSON file readable by everyone\n",
    "    os.chmod(dest, stat.S_IREAD | stat.S_IWRITE | stat.S_IRGRP | stat.S_IROTH)\n",
    "\n",
    "    # make the JSON file's file stat times match those of the .dat\n",
    "    source_file_stat = os.stat(source)\n",
    "    os.utime(dest, (source_file_stat.st_mtime, source_file_stat.st_mtime))\n",
    "\n",
    "    Stat.info('Successfully created %s ' % REPORTING_AREA_LOCATIONS_JSON_FILENAME, host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "    return reporting_areas\n",
    "\n",
    "#jsonify_reporting_area_locations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def jsonify_site_to_reporting_area():\n",
    "    # CSV header:   \"ReportingAreaName\",\"ReportingAreaState\",\"ReportingAreaID\",\"ReportingAreaLat\",\"ReportingAreaLong\",\"SiteID\",\"SiteName\",\"SiteAgencyName\",\"SiteLat\",\"SiteLong\"\n",
    "    field_names = ('name|stateCode|id|lat|lng|siteId|siteName|siteAgencyName|siteLat|siteLng').split('|')\n",
    "\n",
    "    reporting_areas = {}\n",
    "\n",
    "    source = AirnowCommon.DATA_DIRECTORY + '/' + SITE_TO_REPORTING_AREA_CSV_FILENAME\n",
    "    dest = AirnowCommon.DATA_DIRECTORY + '/' + REPORTING_AREAS_TO_SITES_JSON_FILENAME\n",
    "\n",
    "    with open(source, 'r', encoding='cp437') as f:\n",
    "        reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "\n",
    "        line_number = 0\n",
    "        for row in reader:\n",
    "            # skip the header\n",
    "            if line_number > 0:\n",
    "                fields = list(map(lambda s: s.strip(), row))    # strip whitespace from every field\n",
    "                if len(field_names) != len(fields):\n",
    "                    Stat.warning('Record has %d field names but %d fields. Skipping.' % (len(field_names), len(fields)), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "                    continue\n",
    "                field_map = dict(zip(field_names, fields))\n",
    "                key = field_map['id']\n",
    "\n",
    "                if key and len(key) > 0:\n",
    "                    # remember the site ID so we can add to the siteIDs collection later\n",
    "                    site_id = field_map['siteId']\n",
    "\n",
    "                    # see whether we've already seen this reporting area, inserting into the dictionary if not\n",
    "                    if key not in reporting_areas:\n",
    "                        # delete keys we don't need\n",
    "                        field_map.pop('siteId', None)\n",
    "                        field_map.pop('siteName', None)\n",
    "                        field_map.pop('siteAgencyName', None)\n",
    "                        field_map.pop('siteLat', None)\n",
    "                        field_map.pop('siteLng', None)\n",
    "\n",
    "                        # convert lat/lng from string to float\n",
    "                        field_map['lat'] = float(field_map['lat'])\n",
    "                        field_map['lng'] = float(field_map['lng'])\n",
    "\n",
    "                        # add a siteIDs field\n",
    "                        field_map['siteIDs'] = []\n",
    "\n",
    "                        # insert into the dictionary\n",
    "                        reporting_areas[key] = field_map\n",
    "\n",
    "                    reporting_areas[key]['siteIDs'].append(site_id)\n",
    "\n",
    "                else:\n",
    "                    Stat.warning('skipping reporting area [%s, %s] since it has an empty ID' % (field_map['name'], field_map['stateCode']), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "            line_number = line_number + 1\n",
    "\n",
    "    Stat.debug('Read %d reporting areas from %s' % (len(reporting_areas), source), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "    # write the JSON file to disk\n",
    "    tmp = dest + '.tmp' + str(os.getpid())\n",
    "    os.makedirs(os.path.dirname(tmp), exist_ok=True)\n",
    "    with open(tmp, 'w') as json_file:\n",
    "        json.dump(reporting_areas, json_file, sort_keys=True)\n",
    "    os.rename(tmp, dest)\n",
    "\n",
    "    # make the JSON file readable by everyone\n",
    "    os.chmod(dest, stat.S_IREAD | stat.S_IWRITE | stat.S_IRGRP | stat.S_IROTH)\n",
    "\n",
    "    # make the JSON file's file stat times match those of the .dat\n",
    "    source_file_stat = os.stat(source)\n",
    "    os.utime(dest, (source_file_stat.st_mtime, source_file_stat.st_mtime))\n",
    "\n",
    "    Stat.info('Successfully created %s ' % REPORTING_AREAS_TO_SITES_JSON_FILENAME, host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "    return reporting_areas\n",
    "\n",
    "#jsonify_site_to_reporting_area()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "def merge_data():\n",
    "    reporting_area_locations = jsonify_reporting_area_locations()\n",
    "    reporting_areas_to_sites = jsonify_site_to_reporting_area()\n",
    "\n",
    "    # we're going to merge reporting_areas_to_sites into reporting_area_locations, so start by iterating over \n",
    "    # reporting_area_locations and inserting a siteIDs field (an empty array) into each one\n",
    "    for key in reporting_area_locations.keys():\n",
    "        reporting_area_locations[key]['siteIDs'] = []\n",
    "\n",
    "    # now iterate over all the reporting_areas_to_sites, copying the siteIDs over. In the (rare?) case that an\n",
    "    # item exists in reporting_areas_to_sites but not in reporting_area_locations, then we'll insert and copy\n",
    "    # what info we do have about from reporting_areas_to_sites.\n",
    "    for key in reporting_areas_to_sites.keys():\n",
    "        if key in reporting_area_locations:\n",
    "            if reporting_area_locations[key]['lat'] != reporting_areas_to_sites[key]['lat']:\n",
    "                Stat.warning('Ignoring latitude mismatch for reporting area [%s]' % (key), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "            if reporting_area_locations[key]['lng'] != reporting_areas_to_sites[key]['lng']:\n",
    "                Stat.warning('Ignoring longitude mismatch for reporting area [%s]' % (key), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "            if reporting_area_locations[key]['stateCode'] != reporting_areas_to_sites[key]['stateCode']:\n",
    "                Stat.warning('Ignoring stateCode mismatch for reporting area [%s]' % (key), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "            reporting_area_locations[key]['siteIDs'] = reporting_areas_to_sites[key]['siteIDs']\n",
    "        else:\n",
    "            print(\"### Reporting area [%s] not found in reporting_area_locations!\")\n",
    "            reporting_area_locations[key] = reporting_areas_to_sites[key]\n",
    "\n",
    "    # write the JSON file to disk\n",
    "    dest = AirnowCommon.DATA_DIRECTORY + '/' + REPORTING_AREAS_JSON_FILENAME\n",
    "    tmp = dest + '.tmp' + str(os.getpid())\n",
    "    os.makedirs(os.path.dirname(tmp), exist_ok=True)\n",
    "    with open(tmp, 'w') as json_file:\n",
    "        json.dump(reporting_area_locations, json_file, sort_keys=True)\n",
    "    os.rename(tmp, dest)\n",
    "\n",
    "    # make the JSON file readable by everyone\n",
    "    os.chmod(dest, stat.S_IREAD | stat.S_IWRITE | stat.S_IRGRP | stat.S_IROTH)\n",
    "\n",
    "    Stat.info('Successfully created %s ' % REPORTING_AREAS_JSON_FILENAME, host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "#merge_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# For mirroring files located under https://files.airnowtech.org/airnow/today/\n",
    "def mirror_today_file(filename):\n",
    "    Stat.info('Mirroring https://files.airnowtech.org/airnow/today/%s' % (filename), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "    (is_new, message, status_code) = AirnowCommon.mirror_airnow_file('today' + '/' + filename, AirnowCommon.DATA_DIRECTORY + '/' + filename)\n",
    "\n",
    "    if is_new:\n",
    "        Stat.info(message, host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "        return True\n",
    "    else:\n",
    "        if status_code == 304:\n",
    "            Stat.info(message, host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "        elif status_code < 400:\n",
    "            Stat.info(message, host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "        else:\n",
    "            Stat.warning(message, host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "    return False\n",
    "\n",
    "#mirror_today_file(REPORTING_AREA_LOCATIONS_DAT_FILENAME)\n",
    "#mirror_today_file(SITE_TO_REPORTING_AREA_CSV_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def mirror():\n",
    "    starting_timestamp = datetime.datetime.now().timestamp()\n",
    "\n",
    "    # Latest file is at https://files.airnowtech.org/airnow/today/reporting_area_locations_V2.dat\n",
    "    is_new1 = mirror_today_file(REPORTING_AREA_LOCATIONS_DAT_FILENAME)\n",
    "    is_new2 = mirror_today_file(SITE_TO_REPORTING_AREA_CSV_FILENAME)\n",
    "    if is_new1 or is_new2:\n",
    "        merge_data()\n",
    "    else:\n",
    "        Stat.info(\"Files unchanged, nothing to do.\", host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "    elapsed_seconds = datetime.datetime.now().timestamp() - starting_timestamp\n",
    "    Stat.up('Done! (elapsed time: %d seconds)' % (elapsed_seconds), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME, valid_for_secs=MIRROR_TIME_PERIOD_SECS*1.5)\n",
    "\n",
    "def mirror_forever():\n",
    "    while True:\n",
    "        mirror()\n",
    "        sleep_until_next_period(MIRROR_TIME_PERIOD_SECS)\n",
    "\n",
    "mirror_forever()\n",
    "#mirror()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda Python 3",
   "language": "python",
   "name": "anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}