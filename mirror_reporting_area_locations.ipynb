{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mirror Reporting Area Locations\n",
    "\n",
    "Mirrors two Airnow reporting area data files, converts each to JSON, and then merges the two into `reporting_areas.json`. It then mirrors those reporting areas to ESDR (ESDR product [airnow_reporting_area](https://esdr.cmucreatelab.org/api/v1/products/airnow_reporting_area)]), creating any new ones as necessary, and finally caches the ESDR devices and feeds for the `airnow_reporting_area` product as a JSON file named `esdr_reporting_area_devices_and_feeds.json`.\n",
    "\n",
    "It also creates another JSON file, `reporting_area_id_lookup.json`, which is simply a large JSON map of `STATE_CODE|NAME` to reporting area ID (e.g. \"`PA|Liberty-Clairton Area`\" --> \"`pa005`\").  This JSON file is useful for quickly processing the Airnow `reportingarea.dat` files, which very unhelpfully reference reporting areas only by name and state (and lat/long, but name and state is unique enough), and not by ID.\n",
    "\n",
    "Details:\n",
    "\n",
    "* The Airnow [Site_To_ReportingArea.csv](https://files.airnowtech.org/airnow/today/Site_To_ReportingArea.csv) is converted into `reporting_areas_to_sites.json`.\n",
    "* The [reporting area locations .dat file](https://files.airnowtech.org/airnow/today/reporting_area_locations_V2.dat) is converted into `reporting_area_locations.json`.\n",
    "\n",
    "We merge the files using the reporting area ID (e.g. `wa019`, `ak010`, etc). When I first started mirroring, I found a few errors such as multiple reporting areas being assigned to the same ID (see the git history for this file).  I contacted Airnow, and they assured me reporting area IDs should be unique and fixed the problems.\n",
    "\n",
    "The resulting merged JSON file, `reporting_areas.json`, is a dictionary mapping reporting area ID (e.g. `wa019`) to data about that reporting area, including a collection of monitoring site IDs associated with that reporting area.  Note that a monitoring site may be associated with more than one reporting area, and that a reporting area may actually have zero associated monitoring sites.\n",
    "\n",
    "Airnow told me that `Site_To_ReportingArea.csv` gets updated twice daily. But, from what I can tell, the `reporting_area_locations_V2.dat` file gets updated twice per hour, at 25 and 55 minutes after the hour.  So this mirror runs on the hour and half hour.\n",
    "\n",
    "Data sheet is located at https://docs.airnowapi.org/docs/ReportingAreaInformationFactSheet.pdf\n",
    "\n",
    "Reports to stat.createlab.org as `Airnow Reporting Area Locations File - Mirror`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, dateutil, re, requests, subprocess, datetime, glob, stat\n",
    "import csv\n",
    "\n",
    "from dateutil import rrule, tz, parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate to load utils.ipynb\n",
    "# See https://github.com/CMU-CREATE-Lab/python-utils/blob/master/utils.ipynb\n",
    "\n",
    "def exec_ipynb(filename_or_url):\n",
    "    nb = (requests.get(filename_or_url).json() if re.match(r'https?:', filename_or_url) else json.load(open(filename_or_url)))\n",
    "    if(nb['nbformat'] >= 4):\n",
    "        src = [''.join(cell['source']) for cell in nb['cells'] if cell['cell_type'] == 'code']\n",
    "    else:\n",
    "        src = [''.join(cell['input']) for cell in nb['worksheets'][0]['cells'] if cell['cell_type'] == 'code']\n",
    "\n",
    "    tmpname = '/tmp/%s-%s-%d.py' % (os.path.basename(filename_or_url),\n",
    "                                    datetime.datetime.now().strftime('%Y%m%d%H%M%S%f'),\n",
    "                                    os.getpid())\n",
    "    src = '\\n\\n\\n'.join(src)\n",
    "    open(tmpname, 'w').write(src)\n",
    "    code = compile(src, tmpname, 'exec')\n",
    "    exec(code, globals())\n",
    "\n",
    "exec_ipynb('./python-utils/utils.ipynb')\n",
    "exec_ipynb('./python-utils/esdr-library.ipynb')\n",
    "exec_ipynb('./airnow-common.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIRROR_TIME_PERIOD_SECS = 60 * 30   # every 30 minutes\n",
    "\n",
    "STAT_SERVICE_NAME = 'Airnow Reporting Area Locations File - Mirror'\n",
    "STAT_HOSTNAME = 'hal21'\n",
    "STAT_SHORTNAME = 'airnow-mirror-reporting-area-locations-file'\n",
    "\n",
    "REPORTING_AREA_LOCATIONS_DAT_FILENAME = 'reporting_area_locations_V2.dat'\n",
    "REPORTING_AREA_LOCATIONS_JSON_FILENAME = 'reporting_area_locations.json'\n",
    "\n",
    "SITE_TO_REPORTING_AREA_CSV_FILENAME = 'Site_To_ReportingArea.csv'\n",
    "REPORTING_AREAS_TO_SITES_JSON_FILENAME = 'reporting_areas_to_sites.json'\n",
    "\n",
    "REPORTING_AREAS_JSON_FILENAME = 'reporting_areas.json'\n",
    "\n",
    "REPORTING_AREA_ID_LOOKUP_JSON_FILENAME = 'reporting_area_id_lookup.json'\n",
    "\n",
    "ESDR_REPORTING_AREA_DEVICES_AND_FEEDS_JSON_FILENAME = 'esdr_reporting_area_devices_and_feeds.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stat.set_service(STAT_SERVICE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esdr = None\n",
    "reporting_area_product = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently unused now that Airnow has fixed the duplications and errors surrounding reporting area IDs (see docs above, and in the git history)\n",
    "#\n",
    "# def create_reporting_area_unique_id(reporting_area_id, reporting_area_name):\n",
    "#     if reporting_area_id and reporting_area_name:\n",
    "#         stripped_id = reporting_area_id.strip()\n",
    "#         clean_name = re.sub(r'[^a-zA-Z0-9]+', '', reporting_area_name) # Strip non-alphanumeric chars\n",
    "#\n",
    "#         if len(stripped_id) > 0 and len(clean_name) > 0:\n",
    "#             return (stripped_id + '-' + clean_name).lower()\n",
    "#\n",
    "#     return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonify_reporting_area_locations():\n",
    "    field_names = ('name|stateCode|countryCode|forecasts|actionDayName|lat|lng|gmtOffset|hasDST|tzLabel|dstzLabel|id|usaToday|forecastSource').split('|')\n",
    "\n",
    "    reporting_areas = {}\n",
    "\n",
    "    # The file may have non-ASCII characters, in the archaic Original IBM PC 8-bit charset\n",
    "    # known today as Code page 437.  Translate to unicode during read\n",
    "    source = AirnowCommon.DATA_DIRECTORY + '/' + REPORTING_AREA_LOCATIONS_DAT_FILENAME\n",
    "    dest = AirnowCommon.DATA_DIRECTORY + '/' + REPORTING_AREA_LOCATIONS_JSON_FILENAME\n",
    "    data =  open(source, 'r', encoding='cp437').read()\n",
    "\n",
    "    for line in data.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        fields = list(map(lambda s: s.strip(), line.split('|')))    # split on | then strip whitespace from every field\n",
    "        if len(field_names) != len(fields):\n",
    "            Stat.warning('Record has %d field names but %d fields. Skipping.' % (len(field_names), len(fields)), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "            continue\n",
    "        field_map = dict(zip(field_names, fields))\n",
    "        key = field_map['id']\n",
    "\n",
    "        if key and len(key) > 0:\n",
    "            # delete keys we don't need\n",
    "            field_map.pop('forecasts', None)\n",
    "            field_map.pop('actionDayName', None)\n",
    "            field_map.pop('usaToday', None)\n",
    "            field_map.pop('forecasts', None)\n",
    "            field_map.pop('forecastSource', None)\n",
    "\n",
    "            if field_map['hasDST'] == 'Yes':\n",
    "                field_map['hasDST'] = True\n",
    "            elif field_map['hasDST'] == 'No':\n",
    "                field_map['hasDST'] = False\n",
    "                field_map.pop('dstzLabel', None)  # no point including the daylight savings time label if they don't do DST\n",
    "\n",
    "            # convert lat/lng from string to float\n",
    "            field_map['lat'] = float(field_map['lat'])\n",
    "            field_map['lng'] = float(field_map['lng'])\n",
    "\n",
    "            # add it to the map\n",
    "            if key not in reporting_areas:\n",
    "                reporting_areas[key] = field_map\n",
    "            else:\n",
    "                Stat.warning('skipping duplicate ID [%s] for reporting area [%s, %s, %s]' % (key, field_map['name'], field_map['stateCode'], field_map['countryCode']), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "        else:\n",
    "            Stat.warning('skipping reporting area [%s, %s, %s] since it has an empty ID' % (field_map['name'], field_map['stateCode'], field_map['countryCode']), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "    Stat.debug('Read %d reporting areas from %s' % (len(reporting_areas), source), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "    # write the JSON file to disk\n",
    "    tmp = dest + '.tmp' + str(os.getpid())\n",
    "    os.makedirs(os.path.dirname(tmp), exist_ok=True)\n",
    "    with open(tmp, 'w') as json_file:\n",
    "        json.dump(reporting_areas, json_file, sort_keys=True)\n",
    "    os.rename(tmp, dest)\n",
    "\n",
    "    # make the JSON file readable by everyone\n",
    "    os.chmod(dest, stat.S_IREAD | stat.S_IWRITE | stat.S_IRGRP | stat.S_IROTH)\n",
    "\n",
    "    # make the JSON file's file stat times match those of the .dat\n",
    "    source_file_stat = os.stat(source)\n",
    "    os.utime(dest, (source_file_stat.st_mtime, source_file_stat.st_mtime))\n",
    "\n",
    "    Stat.info('Successfully created %s ' % REPORTING_AREA_LOCATIONS_JSON_FILENAME, host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "    return reporting_areas\n",
    "\n",
    "#jsonify_reporting_area_locations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonify_site_to_reporting_area():\n",
    "    # CSV header:   \"ReportingAreaName\",\"ReportingAreaState\",\"ReportingAreaID\",\"ReportingAreaLat\",\"ReportingAreaLong\",\"SiteID\",\"SiteName\",\"SiteAgencyName\",\"SiteLat\",\"SiteLong\"\n",
    "    field_names = ('name|stateCode|id|lat|lng|siteId|siteName|siteAgencyName|siteLat|siteLng').split('|')\n",
    "\n",
    "    reporting_areas = {}\n",
    "\n",
    "    source = AirnowCommon.DATA_DIRECTORY + '/' + SITE_TO_REPORTING_AREA_CSV_FILENAME\n",
    "    dest = AirnowCommon.DATA_DIRECTORY + '/' + REPORTING_AREAS_TO_SITES_JSON_FILENAME\n",
    "\n",
    "    with open(source, 'r', encoding='cp437') as f:\n",
    "        reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "\n",
    "        line_number = 0\n",
    "        for row in reader:\n",
    "            # skip the header\n",
    "            if line_number > 0:\n",
    "                fields = list(map(lambda s: s.strip(), row))    # strip whitespace from every field\n",
    "                if len(field_names) != len(fields):\n",
    "                    Stat.warning('Record has %d field names but %d fields. Skipping.' % (len(field_names), len(fields)), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "                    continue\n",
    "                field_map = dict(zip(field_names, fields))\n",
    "                key = field_map['id']\n",
    "\n",
    "                if key and len(key) > 0:\n",
    "                    # remember the site ID so we can add to the siteIDs collection later\n",
    "                    site_id = field_map['siteId']\n",
    "\n",
    "                    # see whether we've already seen this reporting area, inserting into the dictionary if not\n",
    "                    if key not in reporting_areas:\n",
    "                        # delete keys we don't need\n",
    "                        field_map.pop('siteId', None)\n",
    "                        field_map.pop('siteName', None)\n",
    "                        field_map.pop('siteAgencyName', None)\n",
    "                        field_map.pop('siteLat', None)\n",
    "                        field_map.pop('siteLng', None)\n",
    "\n",
    "                        # convert lat/lng from string to float\n",
    "                        field_map['lat'] = float(field_map['lat'])\n",
    "                        field_map['lng'] = float(field_map['lng'])\n",
    "\n",
    "                        # add a siteIDs field\n",
    "                        field_map['siteIDs'] = []\n",
    "\n",
    "                        # insert into the dictionary\n",
    "                        reporting_areas[key] = field_map\n",
    "\n",
    "                    reporting_areas[key]['siteIDs'].append(site_id)\n",
    "\n",
    "                else:\n",
    "                    Stat.warning('skipping reporting area [%s, %s] since it has an empty ID' % (field_map['name'], field_map['stateCode']), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "            line_number = line_number + 1\n",
    "\n",
    "    Stat.debug('Read %d reporting areas from %s' % (len(reporting_areas), source), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "    # write the JSON file to disk\n",
    "    tmp = dest + '.tmp' + str(os.getpid())\n",
    "    os.makedirs(os.path.dirname(tmp), exist_ok=True)\n",
    "    with open(tmp, 'w') as json_file:\n",
    "        json.dump(reporting_areas, json_file, sort_keys=True)\n",
    "    os.rename(tmp, dest)\n",
    "\n",
    "    # make the JSON file readable by everyone\n",
    "    os.chmod(dest, stat.S_IREAD | stat.S_IWRITE | stat.S_IRGRP | stat.S_IROTH)\n",
    "\n",
    "    # make the JSON file's file stat times match those of the .dat\n",
    "    source_file_stat = os.stat(source)\n",
    "    os.utime(dest, (source_file_stat.st_mtime, source_file_stat.st_mtime))\n",
    "\n",
    "    Stat.info('Successfully created %s ' % REPORTING_AREAS_TO_SITES_JSON_FILENAME, host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "    return reporting_areas\n",
    "\n",
    "#jsonify_site_to_reporting_area()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data():\n",
    "    reporting_area_locations = jsonify_reporting_area_locations()\n",
    "    reporting_areas_to_sites = jsonify_site_to_reporting_area()\n",
    "\n",
    "    # we're going to merge reporting_areas_to_sites into reporting_area_locations, so start by iterating over \n",
    "    # reporting_area_locations and inserting a siteIDs field (an empty array) into each one\n",
    "    for key in reporting_area_locations.keys():\n",
    "        reporting_area_locations[key]['siteIDs'] = []\n",
    "\n",
    "    # now iterate over all the reporting_areas_to_sites, copying the siteIDs over. In the (rare?) case that an\n",
    "    # item exists in reporting_areas_to_sites but not in reporting_area_locations, then we'll insert and copy\n",
    "    # what info we do have about from reporting_areas_to_sites.\n",
    "    for key in reporting_areas_to_sites.keys():\n",
    "        if key in reporting_area_locations:\n",
    "            if reporting_area_locations[key]['lat'] != reporting_areas_to_sites[key]['lat']:\n",
    "                Stat.warning('Ignoring latitude mismatch for reporting area [%s]' % (key), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "            if reporting_area_locations[key]['lng'] != reporting_areas_to_sites[key]['lng']:\n",
    "                Stat.warning('Ignoring longitude mismatch for reporting area [%s]' % (key), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "            if reporting_area_locations[key]['stateCode'] != reporting_areas_to_sites[key]['stateCode']:\n",
    "                Stat.warning('Ignoring stateCode mismatch for reporting area [%s]' % (key), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "            reporting_area_locations[key]['siteIDs'] = reporting_areas_to_sites[key]['siteIDs']\n",
    "        else:\n",
    "            print(\"### Reporting area [%s] not found in reporting_area_locations!\" % key)\n",
    "            reporting_area_locations[key] = reporting_areas_to_sites[key]\n",
    "\n",
    "    # now that reporting_area_locations contains a comprehensive collection of all reporting areas, iterate\n",
    "    # through it and create the reporting_area_id_lookup dictionary\n",
    "    reporting_area_id_lookup = {}\n",
    "    for key in reporting_area_locations:\n",
    "        stateCode = reporting_area_locations[key]['stateCode']\n",
    "        cityName = reporting_area_locations[key]['name']\n",
    "        reporting_area_id_lookup[stateCode + '|' + cityName] = key\n",
    "\n",
    "    # write the REPORTING_AREAS_JSON_FILENAME JSON file to disk\n",
    "    dest = AirnowCommon.DATA_DIRECTORY + '/' + REPORTING_AREAS_JSON_FILENAME\n",
    "    tmp = dest + '.tmp' + str(os.getpid())\n",
    "    os.makedirs(os.path.dirname(tmp), exist_ok=True)\n",
    "    with open(tmp, 'w') as json_file:\n",
    "        json.dump(reporting_area_locations, json_file, sort_keys=True)\n",
    "    os.rename(tmp, dest)\n",
    "\n",
    "    # make the JSON file readable by everyone\n",
    "    os.chmod(dest, stat.S_IREAD | stat.S_IWRITE | stat.S_IRGRP | stat.S_IROTH)\n",
    "\n",
    "    Stat.info('Successfully created %s ' % REPORTING_AREAS_JSON_FILENAME, host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "    # write the REPORTING_AREA_ID_LOOKUP_JSON_FILENAME JSON file to disk\n",
    "    dest = AirnowCommon.DATA_DIRECTORY + '/' + REPORTING_AREA_ID_LOOKUP_JSON_FILENAME\n",
    "    tmp = dest + '.tmp' + str(os.getpid())\n",
    "    os.makedirs(os.path.dirname(tmp), exist_ok=True)\n",
    "    with open(tmp, 'w') as json_file:\n",
    "        json.dump(reporting_area_id_lookup, json_file, sort_keys=True)\n",
    "    os.rename(tmp, dest)\n",
    "\n",
    "    # make the JSON file readable by everyone\n",
    "    os.chmod(dest, stat.S_IREAD | stat.S_IWRITE | stat.S_IRGRP | stat.S_IROTH)\n",
    "\n",
    "    Stat.info('Successfully created %s ' % REPORTING_AREA_ID_LOOKUP_JSON_FILENAME, host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "#merge_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reporting_area_product():\n",
    "    try:\n",
    "        global esdr, reporting_area_product\n",
    "        if not esdr:\n",
    "            esdr = Esdr('esdr-auth-airnow-uploader.json', user_agent='esdr-library.py['+STAT_SERVICE_NAME+']')\n",
    "        if not reporting_area_product:\n",
    "            reporting_area_product = esdr.get_product_by_name('airnow_reporting_area')\n",
    "        return reporting_area_product\n",
    "    except requests.HTTPError as e:\n",
    "        Stat.warning('Failed to get Airnow Reporting Area ESDR product due to error: %s' % (str(e)), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "        return None\n",
    "\n",
    "#get_reporting_area_product()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esdr_reporting_area_devices_and_feeds_cached = None\n",
    "\n",
    "def refresh_esdr_reporting_area_devices_and_feeds_cache():\n",
    "    global esdr, reporting_area_product, esdr_reporting_area_devices_and_feeds_cached\n",
    "    if not esdr:\n",
    "        esdr = Esdr('esdr-auth-airnow-uploader.json', user_agent='esdr-library.py['+STAT_SERVICE_NAME+']')\n",
    "    if not reporting_area_product:\n",
    "        reporting_area_product = get_reporting_area_product()\n",
    "\n",
    "    if not reporting_area_product:\n",
    "        Stat.warning('No Airnow Reporting Area ESDR product found!', host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "        return False\n",
    "\n",
    "    # get all ESDR devices belonging to the Airnow Reporting Area product, dealing with multiple pages of data if necessary\n",
    "    devices = []\n",
    "    while True:\n",
    "        try:\n",
    "            response = esdr.api('GET', '/api/v1/devices', {'where':'productId='+str(reporting_area_product['id']), 'fields':'id,name,serialNumber', 'offset':len(devices)})\n",
    "            if 'data' in response and 'rows' in response['data']:\n",
    "                new_rows = response['data']['rows']\n",
    "                devices.extend(new_rows)\n",
    "                if len(devices) == response['data']['totalCount'] or len(new_rows) <= 0:\n",
    "                    break\n",
    "            else:\n",
    "                raise Exception(\"No data in response when fetching ESDR devices\")\n",
    "        except requests.HTTPError as e:\n",
    "            Stat.warning('Failed to fetch devices for %s due to error: %s' % (ESDR_REPORTING_AREA_DEVICES_AND_FEEDS_JSON_FILENAME, str(e)), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            Stat.warning('Failed to fetch devices for %s due to error: %s' % (ESDR_REPORTING_AREA_DEVICES_AND_FEEDS_JSON_FILENAME, str(e)), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "            return False\n",
    "\n",
    "    # get all ESDR feeds belonging to the Airnow Reporting Area product, dealing with multiple pages of data if necessary\n",
    "    feeds = []\n",
    "    while True:\n",
    "        try:\n",
    "            response = esdr.api('GET', '/api/v1/feeds', {'where':'productId='+str(reporting_area_product['id']), 'fields':'id,deviceId,latitude,longitude', 'orderBy':'deviceId,-id', 'offset':len(feeds)})\n",
    "            if 'data' in response and 'rows' in response['data']:\n",
    "                new_rows = response['data']['rows']\n",
    "                feeds.extend(new_rows)\n",
    "                if len(feeds) == response['data']['totalCount'] or len(new_rows) <= 0:\n",
    "                    break\n",
    "            else:\n",
    "                raise Exception(\"No data in response when fetching ESDR feeds\")\n",
    "        except requests.HTTPError as e:\n",
    "            Stat.warning('Failed to fetch feeds for %s due to error: %s' % (ESDR_REPORTING_AREA_DEVICES_AND_FEEDS_JSON_FILENAME, str(e)), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            Stat.warning('Failed to fetch feeds for %s due to error: %s' % (ESDR_REPORTING_AREA_DEVICES_AND_FEEDS_JSON_FILENAME, str(e)), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "            return False\n",
    "\n",
    "    # create a map, which maps serial number to device id and name\n",
    "    esdr_device_map = {}\n",
    "    device_id_to_serial_number_map = {}\n",
    "    for device in devices:\n",
    "        esdr_device_map[device['serialNumber']] = {'id':device['id'], 'name':device['name'], 'feeds':[]}\n",
    "        device_id_to_serial_number_map[device['id']] = device['serialNumber']\n",
    "\n",
    "    # iterate over all the feeds and insert into the esdr_device_map\n",
    "    for feed in feeds:\n",
    "        device_id = feed['deviceId']\n",
    "        if device_id in device_id_to_serial_number_map:\n",
    "            serial_number = device_id_to_serial_number_map[device_id]\n",
    "            device = esdr_device_map[serial_number]\n",
    "            device['feeds'].append({'id': feed['id'], 'lat': feed['latitude'], 'lng': feed['longitude']})\n",
    "        else:\n",
    "            Stat.warning('Device ID [%s] not found in device_id_to_serial_number_map!' % (device_id), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "            return False\n",
    "\n",
    "    # now write the JSON file\n",
    "    json_dest = AirnowCommon.DATA_DIRECTORY + '/' + ESDR_REPORTING_AREA_DEVICES_AND_FEEDS_JSON_FILENAME\n",
    "    tmp = json_dest + '.tmp' + str(os.getpid())\n",
    "    os.makedirs(os.path.dirname(tmp), exist_ok=True)\n",
    "    with open(tmp, 'w') as json_file:\n",
    "        json.dump(esdr_device_map, json_file, sort_keys=True)\n",
    "    os.rename(tmp, json_dest)\n",
    "\n",
    "    # make the JSON file readable by everyone\n",
    "    os.chmod(json_dest, stat.S_IREAD | stat.S_IWRITE | stat.S_IRGRP | stat.S_IROTH)\n",
    "\n",
    "    Stat.info('Successfully cached %d devices and %d feeds to %s' % (len(devices), len(feeds), ESDR_REPORTING_AREA_DEVICES_AND_FEEDS_JSON_FILENAME), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "    # update the global in-memory cache\n",
    "    esdr_reporting_area_devices_and_feeds_cached = esdr_device_map;\n",
    "\n",
    "    return True\n",
    "\n",
    "#refresh_esdr_reporting_area_devices_and_feeds_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now compare the reporting areas from Airnow with what we have in ESDR, creating any ESDR device+feed necessary (as will be when new reporting areas are added).\n",
    "# WARNING: this assumes that the global in-memory cache esdr_reporting_area_devices_and_feeds_cached is up to date!  That is, you MUST have called \n",
    "# refresh_esdr_reporting_area_devices_and_feeds_cache() before calling this function!\n",
    "def add_any_missing_reporting_areas_to_esdr():\n",
    "    global esdr, reporting_area_product, esdr_reporting_area_devices_and_feeds_cached\n",
    "    if not esdr:\n",
    "        esdr = Esdr('esdr-auth-airnow-uploader.json', user_agent='esdr-library.py['+STAT_SERVICE_NAME+']')\n",
    "    if not reporting_area_product:\n",
    "        reporting_area_product = get_reporting_area_product()\n",
    "\n",
    "    if not reporting_area_product:\n",
    "        Stat.warning('No Airnow Reporting Area ESDR product found!', host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "        return False\n",
    "\n",
    "    was_successful = True\n",
    "\n",
    "    # First read the reporting areas JSON created by merge_data()\n",
    "    with open(AirnowCommon.DATA_DIRECTORY + '/' + REPORTING_AREAS_JSON_FILENAME, 'r') as f:\n",
    "        reporting_areas_by_id = json.load(f)\n",
    "\n",
    "        # For every reporting area, make sure there's a corresponding device+feed in ESDR.  Do so by iterating over all the\n",
    "        # reporting areas found in the JSON resulting from the two merged Airnow reporting area files (i.e. REPORTING_AREAS_JSON_FILENAME).\n",
    "        # For each one, if there's no ESDR device with a serial number corresponding to the reporting area ID, then create the device\n",
    "        # and feed in ESDR.\n",
    "        for id in reporting_areas_by_id:\n",
    "            if id not in esdr_reporting_area_devices_and_feeds_cached:\n",
    "                reporting_area = reporting_areas_by_id[id]\n",
    "\n",
    "                try:\n",
    "                    # build the device and feed name\n",
    "                    device_name = f\"{reporting_area['name']}, {reporting_area['stateCode']}\"\n",
    "                    feed_name = f\"{device_name} [{id}]\"\n",
    "\n",
    "                    # create the device\n",
    "                    device = esdr.get_or_create_device(reporting_area_product, serial_number=str(id), name=device_name)\n",
    "                    Stat.info('ESDR device created for reporting area %s' % (id), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "                    if device:\n",
    "                        # Create the feed.  Yeah, I know there are already methods in the ESDR library for creating\n",
    "                        # feeds.  But they all append the product name to the feed name, which I find really annoying\n",
    "                        # in this case because I end up with feed names such as \"Anchorage, AK airnow_reporting_area\".\n",
    "                        fields = {\n",
    "                                    'name': feed_name,\n",
    "                                    'exposure':'outdoor',\n",
    "                                    'isPublic':1,\n",
    "                                    'isMobile':0\n",
    "                                 }\n",
    "                        if reporting_area['lat'] != None:\n",
    "                            fields['latitude'] = reporting_area['lat']\n",
    "                        if reporting_area['lng'] != None:\n",
    "                            fields['longitude'] = reporting_area['lng']\n",
    "\n",
    "                        response = esdr.api('POST', '/api/v1/devices/%d/feeds' % (device['id']), fields)\n",
    "\n",
    "                        if response['code'] == 201:\n",
    "                            Stat.info('ESDR feed created for reporting area %s' % (id), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "                        else:\n",
    "                            Stat.error('Failed to create the ESDR feed for reporting area %s' % (id), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "                            was_successful = False\n",
    "                    else:\n",
    "                        Stat.error('Failed to create the ESDR device for reporting area %s' % (id), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "                        was_successful = False\n",
    "                except requests.HTTPError as e:\n",
    "                    Stat.error('Failed to create device and/or feed for reporting area %s (HTTP %d)' % (id, e.response.status_code), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "                    was_successful = False\n",
    "                except:\n",
    "                    Stat.error('Failed to create device and/or feed for reporting area %s' % (id), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "                    was_successful = False\n",
    "\n",
    "    return was_successful\n",
    "\n",
    "# refresh_esdr_reporting_area_devices_and_feeds_cache()\n",
    "# s = add_any_missing_reporting_areas_to_esdr()\n",
    "# print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For mirroring files located under https://files.airnowtech.org/airnow/today/\n",
    "def mirror_today_file(filename):\n",
    "    Stat.info('Mirroring https://files.airnowtech.org/airnow/today/%s' % (filename), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "    (is_new, message, status_code) = AirnowCommon.mirror_airnow_file('today' + '/' + filename, AirnowCommon.DATA_DIRECTORY + '/' + filename)\n",
    "\n",
    "    if is_new:\n",
    "        Stat.info(message, host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "        return True\n",
    "    else:\n",
    "        if status_code == 304:\n",
    "            Stat.info(message, host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "        elif status_code < 400:\n",
    "            Stat.info(message, host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "        else:\n",
    "            Stat.warning(message, host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "    return False\n",
    "\n",
    "#mirror_today_file(REPORTING_AREA_LOCATIONS_DAT_FILENAME)\n",
    "#mirror_today_file(SITE_TO_REPORTING_AREA_CSV_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mirror():\n",
    "    starting_timestamp = datetime.datetime.now().timestamp()\n",
    "\n",
    "    # Latest file is at https://files.airnowtech.org/airnow/today/reporting_area_locations_V2.dat\n",
    "    is_new1 = mirror_today_file(REPORTING_AREA_LOCATIONS_DAT_FILENAME)\n",
    "    is_new2 = mirror_today_file(SITE_TO_REPORTING_AREA_CSV_FILENAME)\n",
    "    if is_new1 or is_new2:\n",
    "        merge_data()\n",
    "\n",
    "        # now that we've merged the reporting areas, we refresh our cache (both in memory and on disk) of ESDR devices and feeds for the reporting areas\n",
    "        refresh_esdr_reporting_area_devices_and_feeds_cache()\n",
    "\n",
    "        # now make sure ESDR has a device+feed for all of the reporting areas we found in merge_data()\n",
    "        add_any_missing_reporting_areas_to_esdr()\n",
    "\n",
    "        # finally, refresh our cache again, to pick up any new device+feed pairs, and cache in the JSON\n",
    "        refresh_esdr_reporting_area_devices_and_feeds_cache()\n",
    "    else:\n",
    "        Stat.info(\"Files unchanged, nothing to do.\", host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "    elapsed_seconds = datetime.datetime.now().timestamp() - starting_timestamp\n",
    "    Stat.up('Done! (elapsed time: %d seconds)' % (elapsed_seconds), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME, valid_for_secs=MIRROR_TIME_PERIOD_SECS*1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mirror_forever():\n",
    "    while True:\n",
    "        mirror()\n",
    "        sleep_until_next_period(MIRROR_TIME_PERIOD_SECS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mirror_forever()\n",
    "#mirror()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda Python3",
   "language": "python",
   "name": "anaconda3",
   "resource_dir": "/usr/local/share/jupyter/kernels/anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
