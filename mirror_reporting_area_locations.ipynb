{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Mirror Reporting Area Locations\n",
    "\n",
    "Mirrors two Airnow reporting area data files, converts each to JSON, and then also merges the two into `reporting_areas.json`. Details:\n",
    "\n",
    "* The Airnow [Site_To_ReportingArea.csv](https://files.airnowtech.org/airnow/today/Site_To_ReportingArea.csv) is converted into `reporting_areas_to_sites.json`.\n",
    "* The [reporting area locations .dat file](https://files.airnowtech.org/airnow/today/reporting_area_locations_V2.dat) is converted into `reporting_area_locations.json`.\n",
    "\n",
    "NOTE: Both files contain duplicated, conflicting info, so it's hard to tell what to use as a master record.  For example, the `reporting_area_locations_V2.dat` appears to contain some duplicates, omissions, and/or errors, such as these records, both associated with the `wa019` ID:\n",
    "\n",
    "```\n",
    "Yakima|WA|US|No||46.5949|-120.5122|-8|Yes|PST|PDT|wa019|No|Yakima Regional Clean Air Agency\n",
    "Chehalis|WA|US|No||46.6641|-122.9673|-8|Yes|PST|PDT|wa019|No|Washington Department of Ecology\n",
    "```\n",
    "\n",
    "They're similarly duplicated in `Site_To_ReportingArea.csv` (note: it's normal--and expected--for a reporting area to appear multiple times in `Site_To_ReportingArea.csv` since it's a one-to-many mapping of reporting area to monitoring sites...what seems odd is for a single `ReportingAreaID` such as `wa019` to be referenced by two records with different info for the reporting area):\n",
    "\n",
    "```\n",
    "\"Chehalis\",\"WA\",\"wa019\",46.6641,-122.9673,\"530410004\",\"Chehalis-Market Blvd\",\"Washington Department of Ecology\",46.664089,-122.967323\n",
    "\"Yakima\",\"WA\",\"wa019\",46.5949,-120.5122,\"530770009\",\"Yakima-4th Ave\",\"Yakima Regional Clean Air Agency\",46.594952,-120.512283\n",
    "```\n",
    "\n",
    "The \"ak010\" ID is also duplicated `reporting_area_locations_v2.dat`:\n",
    "\n",
    "```\n",
    "Fairbanks-West|AK|US|No||64.8436|-147.8000|-9|No|AKT|ADT|ak010|No|State of Alaska DEC\n",
    "Bethel|AK|US|No||60.7920|-161.7560|-9|Yes|AKT|ADT|ak010|No|State of Alaska DEC\n",
    "```\n",
    "\n",
    "However, there's only a single usage of `ak010` in `Site_To_ReportingArea.csv`:\n",
    "\n",
    "```\n",
    "\"Bethel\",\"AK\",\"ak010\",60.792,-161.756,\"020500001\",\"Bethel\",\"State of Alaska DEC\",60.79583,-161.767\n",
    "```\n",
    "\n",
    "The `Fairbanks-West` reporting area found in `reporting_area_locations_v2.dat` doesn't appear anywhere in `Site_To_ReportingArea.csv`. The closest I've found (by name matching) is the reporting area `ak002`, as shown in these two records:\n",
    "\n",
    "```\n",
    "\"Fairbanks\",\"AK\",\"ak002\",64.8436,-147.7231,\"840020900040\",\"A Street\",\"State of Alaska DEC\",64.845931,-147.693278\n",
    "\"Fairbanks\",\"AK\",\"ak002\",64.8436,-147.7231,\"020900034\",\"NCore\",\"State of Alaska DEC\",64.8458,-147.72727\n",
    "```\n",
    "\n",
    "I'm not sure what to make of this, and have asked Airnow for clarification on the `ReportingAreaID`.  For now, I'm constructing my own unique ID by concatenating `ReportingAreaID` and the reporting area name (lowercased, and with all nonalphanumeric characters removed).\n",
    "\n",
    "The resulting merged JSON file, `reporting_areas.json`, is a dictionary mapping reporting my combo ID (e.g. `wa019-chehalis`) to data about that reporting area, including a collection of monitoring site IDs associated with that reporting area.  Note that a monitoring site may be associated with more than one reporting area.\n",
    "\n",
    "I'm not sure how often Airnow updates `Site_To_ReportingArea.csv`, but the `reporting_area_locations_V2.dat` file gets updated twice per hour, at 25 and 55 minutes after the hour.  This mirror runs on the hour and half hour.\n",
    "\n",
    "Data sheet is located at https://docs.airnowapi.org/docs/ReportingAreaInformationFactSheet.pdf\n",
    "\n",
    "Reports to stat.createlab.org as `Airnow Reporting Area Locations File - Mirror`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import json, os, dateutil, re, requests, subprocess, datetime, glob, stat\n",
    "import csv\n",
    "\n",
    "from dateutil import rrule, tz, parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Boilerplate to load utils.ipynb\n",
    "# See https://github.com/CMU-CREATE-Lab/python-utils/blob/master/utils.ipynb\n",
    "\n",
    "def exec_ipynb(filename_or_url):\n",
    "    nb = (requests.get(filename_or_url).json() if re.match(r'https?:', filename_or_url) else json.load(open(filename_or_url)))\n",
    "    if(nb['nbformat'] >= 4):\n",
    "        src = [''.join(cell['source']) for cell in nb['cells'] if cell['cell_type'] == 'code']\n",
    "    else:\n",
    "        src = [''.join(cell['input']) for cell in nb['worksheets'][0]['cells'] if cell['cell_type'] == 'code']\n",
    "\n",
    "    tmpname = '/tmp/%s-%s-%d.py' % (os.path.basename(filename_or_url),\n",
    "                                    datetime.datetime.now().strftime('%Y%m%d%H%M%S%f'),\n",
    "                                    os.getpid())\n",
    "    src = '\\n\\n\\n'.join(src)\n",
    "    open(tmpname, 'w').write(src)\n",
    "    code = compile(src, tmpname, 'exec')\n",
    "    exec(code, globals())\n",
    "\n",
    "\n",
    "exec_ipynb('./python-utils/utils.ipynb')\n",
    "exec_ipynb('./airnow-common.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "MIRROR_TIME_PERIOD_SECS = 60 * 30   # every 30 minutes\n",
    "\n",
    "STAT_SERVICE_NAME = 'Airnow Reporting Area Locations File - Mirror'\n",
    "STAT_HOSTNAME = 'hal21'\n",
    "STAT_SHORTNAME = 'airnow-mirror-reporting-area-locations-file'\n",
    "\n",
    "REPORTING_AREA_LOCATIONS_DAT_FILENAME = 'reporting_area_locations_V2.dat'\n",
    "REPORTING_AREA_LOCATIONS_JSON_FILENAME = 'reporting_area_locations.json'\n",
    "\n",
    "SITE_TO_REPORTING_AREA_CSV_FILENAME = 'Site_To_ReportingArea.csv'\n",
    "REPORTING_AREAS_TO_SITES_JSON_FILENAME = 'reporting_areas_to_sites.json'\n",
    "\n",
    "REPORTING_AREAS_JSON_FILENAME = 'reporting_areas.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "Stat.set_service(STAT_SERVICE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def create_reporting_area_unique_id(reporting_area_id, reporting_area_name):\n",
    "    if reporting_area_id and reporting_area_name:\n",
    "        stripped_id = reporting_area_id.strip()\n",
    "        clean_name = re.sub(r'[^a-zA-Z0-9]+', '', reporting_area_name) # Strip non-alphanumeric chars\n",
    "\n",
    "        if len(stripped_id) > 0 and len(clean_name) > 0:\n",
    "            return (stripped_id + '-' + clean_name).lower()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def jsonify_reporting_area_locations():\n",
    "    field_names = ('name|stateCode|countryCode|forecasts|actionDayName|lat|lng|gmtOffset|hasDST|tzLabel|dstzLabel|id|usaToday|forecastSource').split('|')\n",
    "\n",
    "    reporting_areas = {}\n",
    "\n",
    "    # The file may have non-ASCII characters, in the archaic Original IBM PC 8-bit charset\n",
    "    # known today as Code page 437.  Translate to unicode during read\n",
    "    source = AirnowCommon.DATA_DIRECTORY + '/' + REPORTING_AREA_LOCATIONS_DAT_FILENAME\n",
    "    dest = AirnowCommon.DATA_DIRECTORY + '/' + REPORTING_AREA_LOCATIONS_JSON_FILENAME\n",
    "    data =  open(source, 'r', encoding='cp437').read()\n",
    "\n",
    "    for line in data.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        fields = list(map(lambda s: s.strip(), line.split('|')))    # split on | then strip whitespace from every field\n",
    "        if len(field_names) != len(fields):\n",
    "            Stat.warning('Record has %d field names but %d fields. Skipping.' % (len(field_names), len(fields)), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "            continue\n",
    "        field_map = dict(zip(field_names, fields))\n",
    "        key = create_reporting_area_unique_id(field_map['id'], field_map['name'])\n",
    "\n",
    "        if key and len(key) > 0:\n",
    "            # delete keys we don't need\n",
    "            field_map.pop('forecasts', None)\n",
    "            field_map.pop('actionDayName', None)\n",
    "            field_map.pop('usaToday', None)\n",
    "            field_map.pop('forecasts', None)\n",
    "            field_map.pop('forecastSource', None)\n",
    "\n",
    "            if field_map['hasDST'] == 'Yes':\n",
    "                field_map['hasDST'] = True\n",
    "            elif field_map['hasDST'] == 'No':\n",
    "                field_map['hasDST'] = False\n",
    "                field_map.pop('dstzLabel', None)  # no point including the daylight savings time label if they don't do DST\n",
    "\n",
    "            # convert lat/lng from string to float\n",
    "            field_map['lat'] = float(field_map['lat'])\n",
    "            field_map['lng'] = float(field_map['lng'])\n",
    "\n",
    "            # add it to the map\n",
    "            if key not in reporting_areas:\n",
    "                reporting_areas[key] = field_map\n",
    "            else:\n",
    "                Stat.warning('skipping duplicate ID [%s] for reporting area [%s, %s, %s]' % (key, field_map['name'], field_map['stateCode'], field_map['countryCode']), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "        else:\n",
    "            Stat.warning('skipping reporting area [%s, %s, %s] since it has an empty ID' % (field_map['name'], field_map['stateCode'], field_map['countryCode']), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "    Stat.debug('Read %d reporting areas from %s' % (len(reporting_areas), source), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "    # write the JSON file to disk\n",
    "    tmp = dest + '.tmp' + str(os.getpid())\n",
    "    os.makedirs(os.path.dirname(tmp), exist_ok=True)\n",
    "    with open(tmp, 'w') as json_file:\n",
    "        json.dump(reporting_areas, json_file, sort_keys=True)\n",
    "    os.rename(tmp, dest)\n",
    "\n",
    "    # make the JSON file readable by everyone\n",
    "    os.chmod(dest, stat.S_IREAD | stat.S_IWRITE | stat.S_IRGRP | stat.S_IROTH)\n",
    "\n",
    "    # make the JSON file's file stat times match those of the .dat\n",
    "    source_file_stat = os.stat(source)\n",
    "    os.utime(dest, (source_file_stat.st_mtime, source_file_stat.st_mtime))\n",
    "\n",
    "    Stat.info('Successfully created %s ' % REPORTING_AREA_LOCATIONS_JSON_FILENAME, host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "    return reporting_areas\n",
    "\n",
    "#jsonify_reporting_area_locations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def jsonify_site_to_reporting_area():\n",
    "    # CSV header:   \"ReportingAreaName\",\"ReportingAreaState\",\"ReportingAreaID\",\"ReportingAreaLat\",\"ReportingAreaLong\",\"SiteID\",\"SiteName\",\"SiteAgencyName\",\"SiteLat\",\"SiteLong\"\n",
    "    field_names = ('name|stateCode|id|lat|lng|siteId|siteName|siteAgencyName|siteLat|siteLng').split('|')\n",
    "\n",
    "    reporting_areas = {}\n",
    "\n",
    "    source = AirnowCommon.DATA_DIRECTORY + '/' + SITE_TO_REPORTING_AREA_CSV_FILENAME\n",
    "    dest = AirnowCommon.DATA_DIRECTORY + '/' + REPORTING_AREAS_TO_SITES_JSON_FILENAME\n",
    "\n",
    "    # newline='',\n",
    "    with open(source, 'r', encoding='cp437') as f:\n",
    "        reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "\n",
    "        lineNumber = 0\n",
    "        for row in reader:\n",
    "            # skip the header\n",
    "            if lineNumber > 0:\n",
    "                fields = list(map(lambda s: s.strip(), row))    # strip whitespace from every field\n",
    "                if len(field_names) != len(fields):\n",
    "                    Stat.warning('Record has %d field names but %d fields. Skipping.' % (len(field_names), len(fields)), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "                    continue\n",
    "                field_map = dict(zip(field_names, fields))\n",
    "                key = create_reporting_area_unique_id(field_map['id'], field_map['name'])\n",
    "\n",
    "                if key and len(key) > 0:\n",
    "                    # remember the site ID so we can add to the siteIDs collection later\n",
    "                    site_id = field_map['siteId']\n",
    "\n",
    "                    # see whether we've already seen this reporting area, inserting into the dictionary if not\n",
    "                    if key not in reporting_areas:\n",
    "                        # delete keys we don't need\n",
    "                        field_map.pop('siteId', None)\n",
    "                        field_map.pop('siteName', None)\n",
    "                        field_map.pop('siteAgencyName', None)\n",
    "                        field_map.pop('siteLat', None)\n",
    "                        field_map.pop('siteLng', None)\n",
    "\n",
    "                        # convert lat/lng from string to float\n",
    "                        field_map['lat'] = float(field_map['lat'])\n",
    "                        field_map['lng'] = float(field_map['lng'])\n",
    "\n",
    "                        # add a siteIDs field\n",
    "                        field_map['siteIDs'] = []\n",
    "\n",
    "                        # insert into the dictionary\n",
    "                        reporting_areas[key] = field_map\n",
    "\n",
    "                    reporting_areas[key]['siteIDs'].append(site_id)\n",
    "\n",
    "                else:\n",
    "                    Stat.warning('skipping reporting area [%s, %s] since it has an empty ID' % (field_map['name'], field_map['stateCode']), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "            lineNumber = lineNumber + 1\n",
    "\n",
    "    Stat.debug('Read %d reporting areas from %s' % (len(reporting_areas), source), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "    # write the JSON file to disk\n",
    "    tmp = dest + '.tmp' + str(os.getpid())\n",
    "    os.makedirs(os.path.dirname(tmp), exist_ok=True)\n",
    "    with open(tmp, 'w') as json_file:\n",
    "        json.dump(reporting_areas, json_file, sort_keys=True)\n",
    "    os.rename(tmp, dest)\n",
    "\n",
    "    # make the JSON file readable by everyone\n",
    "    os.chmod(dest, stat.S_IREAD | stat.S_IWRITE | stat.S_IRGRP | stat.S_IROTH)\n",
    "\n",
    "    # make the JSON file's file stat times match those of the .dat\n",
    "    source_file_stat = os.stat(source)\n",
    "    os.utime(dest, (source_file_stat.st_mtime, source_file_stat.st_mtime))\n",
    "\n",
    "    Stat.info('Successfully created %s ' % REPORTING_AREAS_TO_SITES_JSON_FILENAME, host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "    return reporting_areas\n",
    "\n",
    "#jsonify_site_to_reporting_area()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "def merge_data():\n",
    "    reporting_area_locations = jsonify_reporting_area_locations()\n",
    "    reporting_areas_to_sites = jsonify_site_to_reporting_area()\n",
    "\n",
    "    # we're going to merge reporting_areas_to_sites into reporting_area_locations, so start by iterating over \n",
    "    # reporting_area_locations and inserting a siteIDs field (an empty array) into each one\n",
    "    for key in reporting_area_locations.keys():\n",
    "        reporting_area_locations[key]['siteIDs'] = []\n",
    "\n",
    "    # now iterate over all the reporting_areas_to_sites, copying the siteIDs over. In the (rare?) case that an\n",
    "    # item exists in reporting_areas_to_sites but not in reporting_area_locations, then we'll insert and copy\n",
    "    # what info we do have about from reporting_areas_to_sites.\n",
    "    for key in reporting_areas_to_sites.keys():\n",
    "        if key in reporting_area_locations:\n",
    "            if reporting_area_locations[key]['lat'] != reporting_areas_to_sites[key]['lat']:\n",
    "                Stat.warning('Ignoring latitude mismatch for reporting area [%s]' % (key), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "            if reporting_area_locations[key]['lng'] != reporting_areas_to_sites[key]['lng']:\n",
    "                Stat.warning('Ignoring longitude mismatch for reporting area [%s]' % (key), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "            if reporting_area_locations[key]['stateCode'] != reporting_areas_to_sites[key]['stateCode']:\n",
    "                Stat.warning('Ignoring stateCode mismatch for reporting area [%s]' % (key), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "            reporting_area_locations[key]['siteIDs'] = reporting_areas_to_sites[key]['siteIDs']\n",
    "        else:\n",
    "            print(\"### Reporting area [%s] not found in reporting_area_locations!\")\n",
    "            reporting_area_locations[key] = reporting_areas_to_sites[key]\n",
    "\n",
    "    # write the JSON file to disk\n",
    "    dest = AirnowCommon.DATA_DIRECTORY + '/' + REPORTING_AREAS_JSON_FILENAME\n",
    "    tmp = dest + '.tmp' + str(os.getpid())\n",
    "    os.makedirs(os.path.dirname(tmp), exist_ok=True)\n",
    "    with open(tmp, 'w') as json_file:\n",
    "        json.dump(reporting_area_locations, json_file, sort_keys=True)\n",
    "    os.rename(tmp, dest)\n",
    "\n",
    "    # make the JSON file readable by everyone\n",
    "    os.chmod(dest, stat.S_IREAD | stat.S_IWRITE | stat.S_IRGRP | stat.S_IROTH)\n",
    "\n",
    "    Stat.info('Successfully created %s ' % REPORTING_AREAS_JSON_FILENAME, host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "#merge_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# For mirroring files located under https://files.airnowtech.org/airnow/today/\n",
    "def mirror_today_file(filename):\n",
    "    Stat.info('Mirroring https://files.airnowtech.org/airnow/today/%s' % (filename), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "    (is_new, message, status_code) = AirnowCommon.mirror_airnow_file('today' + '/' + filename, AirnowCommon.DATA_DIRECTORY + '/' + filename)\n",
    "\n",
    "    if is_new:\n",
    "        Stat.info(message, host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "        return True\n",
    "    else:\n",
    "        if status_code == 304:\n",
    "            Stat.info(message, host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "        elif status_code < 400:\n",
    "            Stat.info(message, host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "        else:\n",
    "            Stat.warning(message, host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "    return False\n",
    "\n",
    "#mirror_today_file(REPORTING_AREA_LOCATIONS_DAT_FILENAME)\n",
    "#mirror_today_file(SITE_TO_REPORTING_AREA_CSV_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def mirror():\n",
    "    starting_timestamp = datetime.datetime.now().timestamp()\n",
    "\n",
    "    # Latest file is at https://files.airnowtech.org/airnow/today/reporting_area_locations_V2.dat\n",
    "    is_new1 = mirror_today_file(REPORTING_AREA_LOCATIONS_DAT_FILENAME)\n",
    "    is_new2 = mirror_today_file(SITE_TO_REPORTING_AREA_CSV_FILENAME)\n",
    "    if is_new1 or is_new2:\n",
    "        merge_data()\n",
    "    else:\n",
    "        Stat.info(\"Files unchanged, nothing to do.\", host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "    elapsed_seconds = datetime.datetime.now().timestamp() - starting_timestamp\n",
    "    Stat.up('Done! (elapsed time: %d seconds)' % (elapsed_seconds), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME, valid_for_secs=MIRROR_TIME_PERIOD_SECS*1.5)\n",
    "\n",
    "def mirror_forever():\n",
    "    while True:\n",
    "        mirror()\n",
    "        sleep_until_next_period(MIRROR_TIME_PERIOD_SECS)\n",
    "\n",
    "mirror_forever()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda Python 3",
   "language": "python",
   "name": "anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}