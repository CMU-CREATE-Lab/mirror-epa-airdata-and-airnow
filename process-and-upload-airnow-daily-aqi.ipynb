{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Process and Upload Airnow Daily AQI\n",
    "\n",
    "Processes the Airnow daily AQI files located in `airnow-data/daily-aqi/dat` and uploads to ESDR.  During processing, this script also creates daily JSON files containing a subset of the information contained in the Airnow daily `.dat` files, but in a format more readibly usable by visualizations.\n",
    "\n",
    "Reports to stat.createlab.org as `Airnow Daily AQI - Upload`.\n",
    "\n",
    "Docs for the daily data files are here: https://docs.airnowapi.org/docs/DailyDataFactSheet.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import json, os, dateutil, re, requests, subprocess, datetime, glob, stat\n",
    "\n",
    "from dateutil import rrule, tz, parser\n",
    "from sqlitedict import SqliteDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Boilerplate to load utils.ipynb\n",
    "# See https://github.com/CMU-CREATE-Lab/python-utils/blob/master/utils.ipynb\n",
    "\n",
    "\n",
    "def exec_ipynb(filename_or_url):\n",
    "    nb = (requests.get(filename_or_url).json() if re.match(r'https?:', filename_or_url) else json.load(open(filename_or_url)))\n",
    "    if(nb['nbformat'] >= 4):\n",
    "        src = [''.join(cell['source']) for cell in nb['cells'] if cell['cell_type'] == 'code']\n",
    "    else:\n",
    "        src = [''.join(cell['input']) for cell in nb['worksheets'][0]['cells'] if cell['cell_type'] == 'code']\n",
    "\n",
    "    tmpname = '/tmp/%s-%s-%d.py' % (os.path.basename(filename_or_url),\n",
    "                                    datetime.datetime.now().strftime('%Y%m%d%H%M%S%f'),\n",
    "                                    os.getpid())\n",
    "    src = '\\n\\n\\n'.join(src)\n",
    "    open(tmpname, 'w').write(src)\n",
    "    code = compile(src, tmpname, 'exec')\n",
    "    exec(code, globals())\n",
    "\n",
    "\n",
    "exec_ipynb('./python-utils/utils.ipynb')\n",
    "exec_ipynb('./python-utils/esdr-library.ipynb')\n",
    "exec_ipynb('./airnow-common.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "NUM_FILES_PER_UPLOAD_BATCH = 500\n",
    "\n",
    "STAT_SERVICE_NAME = 'Airnow Daily AQI - Upload'\n",
    "STAT_HOSTNAME = 'hal21'\n",
    "STAT_SHORTNAME = 'airnow-daily-aqi-upload'\n",
    "\n",
    "MIRROR_TIME_PERIOD_SECS = 60 * 60 * 1   # every hour\n",
    "\n",
    "ESDR_MONITORING_SITE_LOCATION_DEVICES_JSON_FILENAME = 'esdr_monitoring_site_location_devices.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "Stat.set_service(STAT_SERVICE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "uploaded_file_timestamps_db = SqliteDict(AirnowCommon.DAILY_AQI_DIRECTORY + '/uploaded_file_timestamps.db', autocommit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "accumulated = {}\n",
    "accumulated_file_timestamps = {}\n",
    "\n",
    "def clear_accumulated():\n",
    "    global accumulated, accumulated_file_timestamps\n",
    "    accumulated = {}\n",
    "    accumulated_file_timestamps = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# given a path to a .dat data file, this returns a path to the corresponding .json data file\n",
    "def build_path_to_json_data_file(src):\n",
    "    # Starting with a path like this: \"../airnow-data/daily/dat/2020021700.dat\", this chops\n",
    "    # off the path and yields something like \"2020021700.dat\"\n",
    "    src_file_name_and_ext = os.path.basename(src)\n",
    "\n",
    "    # now split into filename and extension, e.g. \"2020021700\" and \".dat\"\n",
    "    (src_file_name, ext) = os.path.splitext(src_file_name_and_ext)\n",
    "\n",
    "    # now build up the path to where we want the JSON file, e.g. \"../airnow-data/daily/json/2020021700.json\"\n",
    "    return AirnowCommon.DAILY_AQI_JSON_DIRECTORY + '/' + src_file_name + '.json'\n",
    "\n",
    "\n",
    "#build_path_to_json_data_file(AirnowCommon.DAILY_AQI_DAT_DIRECTORY + '/2020021700.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Info about the daily AQI files is at https://docs.airnowapi.org/docs/DailyDataFactSheet.pdf\n",
    "# Fields:  \"Valid date|AQSID|site name|parameter name|reporting units|value|averaging period|data source|AQI|AQI Category|latitude|longitude|full AQSID with 3 digit country code prefix\"\n",
    "\n",
    "def process_airnow_daily_aqi_file(src):\n",
    "    src_epoch_timestamp = os.path.getmtime(src)\n",
    "    dt = datetime.datetime.strptime(os.path.basename(src), '%Y%m%d%H.dat')\n",
    "    epoch_time = (dt - datetime.datetime(1970, 1, 1)).total_seconds()\n",
    "    Stat.debug('Processing file %s' % src, host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "    json_data = {'site_id_to_channels': {}, 'channel_info': {}}\n",
    "    json_dest = build_path_to_json_data_file(src)\n",
    "\n",
    "    num_records_read = 0\n",
    "    with open(src, 'r', encoding='cp437') as records:\n",
    "        lineno = 0\n",
    "        error_count = 0\n",
    "        for record in records:\n",
    "            lineno += 1\n",
    "            try:\n",
    "                (mmddyy, id, _, param_name, units, value, averaging_period, _, aqi, aqi_category, _, _, _) = record.split('|')\n",
    "                channel_name = ('AVG' if (float(averaging_period) == 24) else 'PEAK') + '_' + (param_name + '_' + units).upper()\n",
    "            except:\n",
    "                sys.stderr.write('Problem parsing %s line %d, skipping\\n' % (src, lineno))\n",
    "                sys.stderr.write('Line \"%s\"\\n' % record)\n",
    "                error_count += 1\n",
    "                continue\n",
    "            channel_name = re.sub(r'\\W', '_', channel_name) # Replace non-word chars with _;  e.g. PM2.5 becomes PM2_5\n",
    "\n",
    "            if id not in accumulated:\n",
    "                accumulated[id] = {}\n",
    "\n",
    "            if channel_name not in accumulated[id]:\n",
    "                accumulated[id][channel_name] = []\n",
    "\n",
    "            if id not in json_data['site_id_to_channels']:\n",
    "                json_data['site_id_to_channels'][id] = {}\n",
    "\n",
    "            # warn if we're overwriting an existing channel name...pretty sure this should never happen\n",
    "            if channel_name in json_data['site_id_to_channels'][id]:\n",
    "                Stat.warning('Found duplicate channel %s for site %s in %s' % (channel_name, id, src), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "            json_data['site_id_to_channels'][id][channel_name]={'val': float(value), 'aqi': float(aqi), 'aqi_cat': float(aqi_category)}\n",
    "\n",
    "            if channel_name not in json_data['channel_info']:\n",
    "                json_data['channel_info'][channel_name] = {\n",
    "                    'param' : param_name,\n",
    "                    'units' : units,\n",
    "                    'avg_period' : averaging_period,\n",
    "                    'sites' : []\n",
    "                }\n",
    "            json_data['channel_info'][channel_name]['sites'].append(id)\n",
    "\n",
    "            accumulated[id][channel_name].append([epoch_time, float(value), float(aqi), float(aqi_category)])\n",
    "            num_records_read += 1\n",
    "        if error_count > 5:\n",
    "            raise Exception('Too many parse errors (%d) reading %s, aborting' % (error_count, src))\n",
    "\n",
    "    if error_count > 0:\n",
    "        Stat.warning('Read %d records from %s (%d error(s))' % (num_records_read, src, error_count), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "    else:\n",
    "        Stat.debug('Read %d records from %s (%d error(s))' % (num_records_read, src, error_count), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "    accumulated_file_timestamps[src] = src_epoch_timestamp\n",
    "\n",
    "    # now write the JSON file\n",
    "    tmp = json_dest + '.tmp' + str(os.getpid())\n",
    "    os.makedirs(os.path.dirname(tmp), exist_ok=True)\n",
    "    with open(tmp, 'w') as json_file:\n",
    "        json.dump(json_data, json_file, sort_keys=True)\n",
    "    os.rename(tmp, json_dest)\n",
    "\n",
    "    # make the JSON file readable by everyone\n",
    "    os.chmod(json_dest, stat.S_IREAD | stat.S_IWRITE | stat.S_IRGRP | stat.S_IROTH)\n",
    "\n",
    "    # make the JSON file's file stat times match those of the .dat\n",
    "    source_file_stat = os.stat(src)\n",
    "    os.utime(json_dest, (source_file_stat.st_mtime, source_file_stat.st_mtime))\n",
    "\n",
    "#process_airnow_daily_aqi_file(AirnowCommon.DAILY_AQI_DAT_DIRECTORY + '/2020022300.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "sites_cached = None\n",
    "def get_site_info(site_id):\n",
    "    global sites_cached\n",
    "    if not sites_cached:\n",
    "        with open(AirnowCommon.DATA_DIRECTORY + '/monitoring_site_locations.json', 'r') as f:\n",
    "            sites_cached = json.load(f)\n",
    "\n",
    "    try:\n",
    "        return sites_cached['sites'][site_id]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# print(json.dumps(get_site_info('420030008'), sort_keys=True, indent=3))  # Lawrenceville aka \"BAPC 301 39TH STREET BLDG #7 AirNow\"\n",
    "# print(json.dumps(get_site_info('000050121'), sort_keys=True, indent=3))  # Meteorological Service of Canada\"\n",
    "# print(json.dumps(get_site_info('044201010'), sort_keys=True, indent=3))  # null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "esdr = None\n",
    "airnow_product = None\n",
    "esdr_monitoring_site_devices = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def get_airnow_product():\n",
    "    global esdr, airnow_product\n",
    "    if not esdr:\n",
    "        esdr = Esdr('esdr-auth-airnow-uploader.json', user_agent='esdr-library.py['+STAT_SERVICE_NAME+']')\n",
    "    if not airnow_product:\n",
    "        # esdr.create_product('AirNow', 'AirNow', 'EPA and Sonoma Tech', 'Real-time feeds from EPA/STI AirNow')\n",
    "        airnow_product = esdr.get_product_by_name('AirNow')\n",
    "    return airnow_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "def get_esdr_monitoring_site_device(serialNumber):\n",
    "    global airnow_product, esdr_monitoring_site_devices\n",
    "    if not airnow_product:\n",
    "        airnow_product = get_airnow_product()\n",
    "    if not esdr_monitoring_site_devices:\n",
    "        with open(AirnowCommon.DATA_DIRECTORY + '/' + ESDR_MONITORING_SITE_LOCATION_DEVICES_JSON_FILENAME, 'r') as f:\n",
    "            esdr_monitoring_site_devices = json.load(f)\n",
    "\n",
    "    if serialNumber in esdr_monitoring_site_devices:\n",
    "        # get a copy of the device\n",
    "        device = esdr_monitoring_site_devices[serialNumber].copy()\n",
    "\n",
    "        # add the serial number and product id\n",
    "        device['serialNumber'] = serialNumber\n",
    "        device['productId'] = airnow_product['id']\n",
    "        return device\n",
    "\n",
    "    return None\n",
    "\n",
    "# print(get_esdr_monitoring_site_device('no such site'))  # None\n",
    "# print(get_esdr_monitoring_site_device('010972005'))     # {'id': 2264, 'name': 'BAYROAD', 'serialNumber': '010972005', 'productId': 11}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def upload_site(site_id):\n",
    "    global esdr, airnow_product\n",
    "    if not esdr:\n",
    "        esdr = Esdr('esdr-auth-airnow-uploader.json', user_agent='esdr-library.py['+STAT_SERVICE_NAME+']')\n",
    "    if not airnow_product:\n",
    "        airnow_product = get_airnow_product()\n",
    "\n",
    "    # try to get the device from the cache\n",
    "    device = get_esdr_monitoring_site_device(site_id)\n",
    "\n",
    "    site_info = get_site_info(site_id)\n",
    "\n",
    "    if not device:\n",
    "        if not site_info:\n",
    "            Stat.warning('Cannot create device for site %s because no information can be found for it.  Skipping.' % (site_id), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "            return\n",
    "        device = esdr.get_or_create_device(airnow_product, serial_number=site_id, name=site_info['site name'])\n",
    "\n",
    "    # find the feed, but give the lat/lon for the case where the site has moved, because esdr.get_feed()\n",
    "    # will match by lat/lon if there are multiple feeds for the device\n",
    "    lat = float(site_info['latitude']) if site_info else None\n",
    "    lon = float(site_info['longitude']) if site_info else None\n",
    "    feed = esdr.get_feed(device, lat=lat, lon=lon)\n",
    "\n",
    "    if not feed:\n",
    "        if not site_info:\n",
    "            Stat.warning('Cannot create feed for site %s because no information can be found for it.  Skipping.' % (site_id), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "            return\n",
    "        feed = esdr.get_or_create_feed(device, lat=lat, lon=lon)\n",
    "\n",
    "    if site_id in accumulated:\n",
    "        channels = accumulated[site_id]\n",
    "\n",
    "        for channel in channels:\n",
    "            try:\n",
    "                esdr.upload(feed, {\n",
    "                    'channel_names': [channel + \"_VALUE\", channel + \"_AQI\", channel + \"_AQI_CATEGORY\"],\n",
    "                    'data': channels[channel]\n",
    "                })\n",
    "                Stat.info('%s/%s, %s: Uploaded %d samples.' % (site_id, device['name'], channel, len(channels[channel])), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "            except requests.HTTPError as e:\n",
    "                Stat.warning('%s/%s, %s: Failed to upload %d samples (HTTP %d).' %\n",
    "                             (site_id, device['name'], channel, len(channels[channel]), e.response.status_code), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "            except:\n",
    "                Stat.warning('%s/%s, %s: Failed to upload %d samples.' %\n",
    "                             (site_id, device['name'], channel, len(channels[channel])), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "    else:\n",
    "        Stat.warning('%s/%s: No accumulated data found. Skipping.' %\n",
    "                             (site_id, device['name']), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "\n",
    "#upload_site('420030008')  # Lawrenceville aka \"BAPC 301 39TH STREET BLDG #7 AirNow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def upload_accumulated():\n",
    "    global uploaded_file_timestamps_db\n",
    "    for site_id in sorted(accumulated.keys()):\n",
    "        upload_site(site_id)\n",
    "    for src in sorted(accumulated_file_timestamps):\n",
    "        uploaded_file_timestamps_db[src] = accumulated_file_timestamps[src]\n",
    "        Stat.debug('Uploaded %s to ESDR' % (src), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME)\n",
    "    clear_accumulated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "def is_unmodified(src):\n",
    "    global uploaded_file_timestamps_db\n",
    "    return os.path.getmtime(src) == uploaded_file_timestamps_db[src]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "def process_all():\n",
    "    starting_timestamp = datetime.datetime.now().timestamp()\n",
    "    clear_accumulated()\n",
    "    data_files = sorted(glob.glob(AirnowCommon.DAILY_AQI_DAT_DIRECTORY + '/[0-9]*.dat'))\n",
    "    Stat.up('Processing %d data files...' % (len(data_files)), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME, valid_for_secs=MIRROR_TIME_PERIOD_SECS*1.5)\n",
    "\n",
    "    for src in data_files:\n",
    "        if len(accumulated_file_timestamps) == NUM_FILES_PER_UPLOAD_BATCH:\n",
    "            upload_accumulated()\n",
    "        try:\n",
    "            if is_unmodified(src):\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        process_airnow_daily_aqi_file(src)\n",
    "    upload_accumulated()\n",
    "    elapsed_seconds = datetime.datetime.now().timestamp() - starting_timestamp\n",
    "\n",
    "    Stat.up('Done processing %d data files (elapsed time: %d seconds)' % (len(data_files), elapsed_seconds), host=STAT_HOSTNAME, shortname=STAT_SHORTNAME, valid_for_secs=MIRROR_TIME_PERIOD_SECS*1.5)\n",
    "\n",
    "def process_all_forever():\n",
    "    while True:\n",
    "        process_all()\n",
    "        sleep_until_next_period(MIRROR_TIME_PERIOD_SECS, 15*60)  # start at 15 minutes after the hour\n",
    "\n",
    "\n",
    "process_all_forever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda Python 3",
   "language": "python",
   "name": "anaconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}